{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the repeat experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = [\"Backdoor.csv\" , \"DDoS_HTTP.csv\" , \"DDoS_ICMP.csv\" , \"DDoS_TCP.csv\" , \"DDoS_UDP.csv\"  , \"Password.csv\" , \"Port_Scanning.csv\" ,\"Ransomware.csv\", \"SQL_injection.csv\" , \"Uploading.csv\" , \"Vulnerability_scanner.csv\" ,\"XSS.csv\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, clfs, p_nodes, gms):\n",
    "    p_x_node = np.zeros((len(x), len(clfs))) # p[x, in node i] = p(x | x in Node i) * p(x in Node i)\n",
    "    p_y_given_node = np.zeros((len(x), len(clfs))) # p[y | x, node i]\n",
    "    for i in range(len(clfs)):\n",
    "        p_x_node[:, i] = p_nodes[i] * np.exp(gms[i].score_samples(x))\n",
    "        p_y_given_node[:, i] = clfs[i].predict_proba(x)[:, 1]\n",
    "    \n",
    "    p_y = p_y_given_node * p_x_node / (np.sum(p_x_node, axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "    p_y = np.hstack([1-np.sum(p_y, axis=-1, keepdims=True), p_y])\n",
    "\n",
    "    return np.argmax(p_y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(input_file)):\n",
    "    data = pd.read_csv(input_file[i])\n",
    "    data['attack_type'] = i + 1\n",
    "    data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "    data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "    if len(data) < 2000:\n",
    "        data_list.append(data)\n",
    "    else:\n",
    "        data0 = data[data['attack_type'].isin([0])]\n",
    "        x1 = data0.sample(n = 1000, random_state = i + 42, axis = 0)\n",
    "        data1 = data[data['attack_type'].isin([i+1])]\n",
    "        x2 = data1.sample(n = 1000, random_state = i + 43, axis = 0)   \n",
    "        data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "        data_list.append(data_selected) \n",
    "for i in range(len(data_list)):\n",
    "    print(len(data_list[i]))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>http.content_length</th>\n",
       "      <th>http.response</th>\n",
       "      <th>tcp.ack</th>\n",
       "      <th>tcp.ack_raw</th>\n",
       "      <th>tcp.checksum</th>\n",
       "      <th>tcp.connection.fin</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.conflag.cleansess</th>\n",
       "      <th>mqtt.conflags</th>\n",
       "      <th>mqtt.hdrflags</th>\n",
       "      <th>mqtt.len</th>\n",
       "      <th>mqtt.msgtype</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1934832499</td>\n",
       "      <td>19721</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24507</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3703765364</td>\n",
       "      <td>46928</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>208620284</td>\n",
       "      <td>44688</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3557485597</td>\n",
       "      <td>42863</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      arp.opcode  arp.hw.size  icmp.checksum  icmp.seq_le  \\\n",
       "1995           0            0              0            0   \n",
       "1996           0            0              0            0   \n",
       "1997           0            0              0            0   \n",
       "1998           0            0              0            0   \n",
       "1999           0            0              0            0   \n",
       "\n",
       "      http.content_length  http.response  tcp.ack  tcp.ack_raw  tcp.checksum  \\\n",
       "1995                    0              0      150   1934832499         19721   \n",
       "1996                    0              0        0            0         24507   \n",
       "1997                    0              0        1   3703765364         46928   \n",
       "1998                    0              0        1    208620284         44688   \n",
       "1999                    0              0        1   3557485597         42863   \n",
       "\n",
       "      tcp.connection.fin  ...  mqtt.conflag.cleansess  mqtt.conflags  \\\n",
       "1995                   0  ...                       0              0   \n",
       "1996                   0  ...                       0              0   \n",
       "1997                   0  ...                       0              0   \n",
       "1998                   0  ...                       0              0   \n",
       "1999                   0  ...                       0              0   \n",
       "\n",
       "      mqtt.hdrflags  mqtt.len  mqtt.msgtype  mqtt.proto_len  mqtt.topic_len  \\\n",
       "1995              0         0             0               0               0   \n",
       "1996              0         0             0               0               0   \n",
       "1997              0         0             0               0               0   \n",
       "1998              0         0             0               0               0   \n",
       "1999              0         0             0               0               0   \n",
       "\n",
       "      mqtt.ver  Attack_label  attack_type  \n",
       "1995         0             1           10  \n",
       "1996         0             1           10  \n",
       "1997         0             1           10  \n",
       "1998         0             1           10  \n",
       "1999         0             1           10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[9].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42 # set the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4a57b494c641089ee5fd9abbc61c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9b54539f3a4fdc826f705d3c49fc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total number is  24000\n",
      "The proportion of the nodes are: [0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      " 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clfs = [] # This is going to contain 14 different classifiers\n",
    "n_samples = []\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "for i in tqdm(range(len(input_file))): # reading the data\n",
    "    data0 = data_list[i]\n",
    "    y[i][data0.attack_type == 0] = 0\n",
    "    n_samples.append(len(y[i])) # the number of this node\n",
    "    x = data_list[i]\n",
    "    x = x.drop(columns='attack_type')\n",
    "    x = x.drop(columns='Attack_label')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = rs)\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_test_list.append(x_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    #classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "    # classifier = SVC(kernel = 'rbf', random_state = rs, gamma='scale', max_iter=-1, probability=True)\n",
    "    # classifier.fit(data_list[i], y[i])\n",
    "    classifier = LogisticRegression(max_iter=10000)\n",
    "    classifier.fit(x_train_list[i], y_train_list[i])\n",
    "    clfs.append(classifier)\n",
    "\n",
    "total_n_samples = np.sum(n_samples)\n",
    "print('total number is ',total_n_samples)\n",
    "p_nodes =  np.array(n_samples) / total_n_samples\n",
    "print('The proportion of the nodes are:', p_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_file)):\n",
    "    x_test_list[i] = scaler.transform(x_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.vstack(x_test_list)\n",
    "y_test = np.hstack(y_test_list)\n",
    "x_train = np.vstack(x_train_list)\n",
    "y_train = np.hstack(y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c46b34610c43a1b1e72420d500f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gms = []\n",
    "K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    x = x_train_list[i]\n",
    "    gm = GaussianMixture(n_components = K).fit(x)  \n",
    "    gms.append(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction = pred(x_test, clfs, p_nodes, gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is: 0.7555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = prediction == y_test\n",
    "accuracy = np.mean(correct)\n",
    "print('Overall accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [0.9818417639429312, 0.7119341563786008, 0.31275720164609055, 0.9958847736625515, 0.7777777777777778, 1.0, 0.12757201646090535, 0.7695473251028807, 0.2962962962962963, 0.13991769547325103, 0.14814814814814814, 0.7777777777777778, 0.13580246913580246]\n",
      "Recalls: [0.7463643086024155, 0.6628352490421456, 0.5467625899280576, 1.0, 0.7297297297297297, 0.8408304498269896, 0.7045454545454546, 0.6678571428571428, 0.8, 0.7391304347826086, 0.6545454545454545, 0.9947368421052631, 0.6875]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[y_test==i]) == 0:\n",
    "        accs.append(0)\n",
    "    else:\n",
    "        accs.append(np.mean(correct[y_test==i]))\n",
    "print('Accuracies:', accs)\n",
    "recalls = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[prediction==i]) == 0:\n",
    "        recalls.append(0)\n",
    "    else:\n",
    "        recalls.append(np.mean(correct[prediction==i]))\n",
    "print('Recalls:', recalls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the test:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary = LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941dabf33b6c476cb54727c761fe47b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7388841021191862\n",
      "0 Precision std= 0.03215557720512144\n",
      "1 Precision mean= 0.6875197958627732\n",
      "1 Precision std= 0.1738185152489118\n",
      "2 Precision mean= 0.6154510861252681\n",
      "2 Precision std= 0.19700052128947293\n",
      "3 Precision mean= 1.0\n",
      "3 Precision std= 0.0\n",
      "4 Precision mean= 0.7236161533757619\n",
      "4 Precision std= 0.05615195334130809\n",
      "5 Precision mean= 1.0\n",
      "5 Precision std= 0.0\n",
      "6 Precision mean= 0.5882672948560528\n",
      "6 Precision std= 0.18970598208380463\n",
      "7 Precision mean= 0.6808462823304577\n",
      "7 Precision std= 0.041437735026658096\n",
      "8 Precision mean= 0.5558465425353457\n",
      "8 Precision std= 0.30079230211237074\n",
      "9 Precision mean= 0.5964192773525371\n",
      "9 Precision std= 0.3243603233482481\n",
      "10 Precision mean= 0.6909129793233917\n",
      "10 Precision std= 0.10758606039130725\n",
      "11 Precision mean= 0.9367959611563638\n",
      "11 Precision std= 0.0666715971965308\n",
      "12 Precision mean= 0.5901058674906204\n",
      "12 Precision std= 0.2788287982297519\n",
      "Recall mean of  0  = 0.8381050836996433\n",
      "Recall std of  0  = 0.16535749756296866\n",
      "Recall mean of  1  = 0.522269089911168\n",
      "Recall std of  1  = 0.17549487509693837\n",
      "Recall mean of  2  = 0.28320993490978885\n",
      "Recall std of  2  = 0.12413632445762633\n",
      "Recall mean of  3  = 1.0\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 0.788505409989526\n",
      "Recall std of  4  = 0.055057918947072064\n",
      "Recall mean of  5  = 0.9995893067970989\n",
      "Recall std of  5  = 0.0012321055831478298\n",
      "Recall mean of  6  = 0.16178913682835755\n",
      "Recall std of  6  = 0.14288484473271576\n",
      "Recall mean of  7  = 0.7135852631834733\n",
      "Recall std of  7  = 0.07023875442828026\n",
      "Recall mean of  8  = 0.2911471468002803\n",
      "Recall std of  8  = 0.16732619565700926\n",
      "Recall mean of  9  = 0.21540703471023778\n",
      "Recall std of  9  = 0.18291505620878695\n",
      "Recall mean of  10  = 0.12631515092163964\n",
      "Recall std of  10  = 0.02844450412191252\n",
      "Recall mean of  11  = 0.7780622132390033\n",
      "Recall std of  11  = 0.0290647471296665\n",
      "Recall mean of  12  = 0.1622589047833167\n",
      "Recall std of  12  = 0.09615269158652161\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = LogisticRegression(random_state=epoch)\n",
    "        #classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = accs\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = recalls\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.89 \\% ( 0.03 ) &\n",
      "68.75 \\% ( 0.17 ) &\n",
      "61.55 \\% ( 0.2 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "72.36 \\% ( 0.06 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "58.83 \\% ( 0.19 ) &\n",
      "68.08 \\% ( 0.04 ) &\n",
      "55.58 \\% ( 0.3 ) &\n",
      "59.64 \\% ( 0.32 ) &\n",
      "69.09 \\% ( 0.11 ) &\n",
      "93.68 \\% ( 0.07 ) &\n",
      "59.01 \\% ( 0.28 ) &\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary = KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423041dd205a4e5aadefb96338f2bded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.741507689496051\n",
      "0 Precision std= 0.01816138052017399\n",
      "1 Precision mean= 0.7388803438632314\n",
      "1 Precision std= 0.17270496227344936\n",
      "2 Precision mean= 0.5920844582584969\n",
      "2 Precision std= 0.1892334181212948\n",
      "3 Precision mean= 1.0\n",
      "3 Precision std= 0.0\n",
      "4 Precision mean= 0.7891732926877979\n",
      "4 Precision std= 0.08641365967154921\n",
      "5 Precision mean= 1.0\n",
      "5 Precision std= 0.0\n",
      "6 Precision mean= 0.6267264582534711\n",
      "6 Precision std= 0.14466020863115606\n",
      "7 Precision mean= 0.652270671324659\n",
      "7 Precision std= 0.05770101050754602\n",
      "8 Precision mean= 0.5725541827806991\n",
      "8 Precision std= 0.21248709217219927\n",
      "9 Precision mean= 0.34411675389821356\n",
      "9 Precision std= 0.2977935151014835\n",
      "10 Precision mean= 0.6572695034045392\n",
      "10 Precision std= 0.18638565685021172\n",
      "11 Precision mean= 0.962273680960766\n",
      "11 Precision std= 0.0338869107395621\n",
      "12 Precision mean= 0.555114385200237\n",
      "12 Precision std= 0.32179480773472635\n",
      "Recall mean of  0  = 0.8704032836227213\n",
      "Recall std of  0  = 0.08442241277211392\n",
      "Recall mean of  1  = 0.5912700996697187\n",
      "Recall std of  1  = 0.16860347485912563\n",
      "Recall mean of  2  = 0.2563346701000694\n",
      "Recall std of  2  = 0.1269194886936607\n",
      "Recall mean of  3  = 0.9995951151544954\n",
      "Recall std of  3  = 0.0012146987788898855\n",
      "Recall mean of  4  = 0.7445705094676166\n",
      "Recall std of  4  = 0.10601593229651801\n",
      "Recall mean of  5  = 0.997407329732531\n",
      "Recall std of  5  = 0.0031851777279906135\n",
      "Recall mean of  6  = 0.10896923782417281\n",
      "Recall std of  6  = 0.024838538001860835\n",
      "Recall mean of  7  = 0.7843043360800406\n",
      "Recall std of  7  = 0.10892521539565805\n",
      "Recall mean of  8  = 0.3769917629710045\n",
      "Recall std of  8  = 0.18081641360588266\n",
      "Recall mean of  9  = 0.24360216907441049\n",
      "Recall std of  9  = 0.12192493992944024\n",
      "Recall mean of  10  = 0.1482020258939499\n",
      "Recall std of  10  = 0.05431735894829291\n",
      "Recall mean of  11  = 0.7711753215211877\n",
      "Recall std of  11  = 0.0427312850889387\n",
      "Recall mean of  12  = 0.1479613939877344\n",
      "Recall std of  12  = 0.0577535564222785\n",
      "Latex Precisions\n",
      "74.15 \\% ( 0.02 ) &\n",
      "73.89 \\% ( 0.17 ) &\n",
      "59.21 \\% ( 0.19 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "78.92 \\% ( 0.09 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "62.67 \\% ( 0.14 ) &\n",
      "65.23 \\% ( 0.06 ) &\n",
      "57.26 \\% ( 0.21 ) &\n",
      "34.41 \\% ( 0.3 ) &\n",
      "65.73 \\% ( 0.19 ) &\n",
      "96.23 \\% ( 0.03 ) &\n",
      "55.51 \\% ( 0.32 ) &\n",
      "Latex Recalls\n",
      "74.15 \\% ( 0.02 ) &\n",
      "73.89 \\% ( 0.17 ) &\n",
      "59.21 \\% ( 0.19 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "78.92 \\% ( 0.09 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "62.67 \\% ( 0.14 ) &\n",
      "65.23 \\% ( 0.06 ) &\n",
      "57.26 \\% ( 0.21 ) &\n",
      "34.41 \\% ( 0.3 ) &\n",
      "65.73 \\% ( 0.19 ) &\n",
      "96.23 \\% ( 0.03 ) &\n",
      "55.51 \\% ( 0.32 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        #classifier = LogisticRegression(random_state=epoch)\n",
    "        classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = accs\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = recalls\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4fca54712d49879179cfb51a537c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7447387161457149\n",
      "0 Precision std= 0.029660753238044495\n",
      "1 Precision mean= 0.7239339189524244\n",
      "1 Precision std= 0.21224032860668135\n",
      "2 Precision mean= 0.6981789688330158\n",
      "2 Precision std= 0.24329846037681863\n",
      "3 Precision mean= 1.0\n",
      "3 Precision std= 0.0\n",
      "4 Precision mean= 0.7565899616901943\n",
      "4 Precision std= 0.07736294100474271\n",
      "5 Precision mean= 0.9654972195737137\n",
      "5 Precision std= 0.06926349511885513\n",
      "6 Precision mean= 0.6662561793470453\n",
      "6 Precision std= 0.22064104079370042\n",
      "7 Precision mean= 0.6647609950545628\n",
      "7 Precision std= 0.053573270613164026\n",
      "8 Precision mean= 0.5497520141316414\n",
      "8 Precision std= 0.2433951675206493\n",
      "9 Precision mean= 0.5622527591601025\n",
      "9 Precision std= 0.3440716143636287\n",
      "10 Precision mean= 0.706724704888271\n",
      "10 Precision std= 0.0998603411824382\n",
      "11 Precision mean= 0.9220472278300467\n",
      "11 Precision std= 0.07121234416238494\n",
      "12 Precision mean= 0.6829345996690833\n",
      "12 Precision std= 0.3280296057640471\n",
      "Recall mean of  0  = 0.8260893247396301\n",
      "Recall std of  0  = 0.16345305949099856\n",
      "Recall mean of  1  = 0.5667532222164552\n",
      "Recall std of  1  = 0.1959966773944598\n",
      "Recall mean of  2  = 0.2649319103247424\n",
      "Recall std of  2  = 0.08871306766925484\n",
      "Recall mean of  3  = 1.0\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 0.7687791015421619\n",
      "Recall std of  4  = 0.08961498647423258\n",
      "Recall mean of  5  = 0.9997959183673469\n",
      "Recall std of  5  = 0.0008895712129674951\n",
      "Recall mean of  6  = 0.12672935662113669\n",
      "Recall std of  6  = 0.08700380499900871\n",
      "Recall mean of  7  = 0.7569507014018508\n",
      "Recall std of  7  = 0.10217825862176713\n",
      "Recall mean of  8  = 0.2920087071920517\n",
      "Recall std of  8  = 0.19509205294367046\n",
      "Recall mean of  9  = 0.24004005567052297\n",
      "Recall std of  9  = 0.20712189712992723\n",
      "Recall mean of  10  = 0.1393014686531489\n",
      "Recall std of  10  = 0.024663071692195167\n",
      "Recall mean of  11  = 0.7802495705714574\n",
      "Recall std of  11  = 0.028555656962605744\n",
      "Recall mean of  12  = 0.16531670928271913\n",
      "Recall std of  12  = 0.1271660823769143\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "        #classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = accs\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = recalls\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex Precisions\n",
      "74.47 \\% ( 0.03 ) &\n",
      "72.39 \\% ( 0.21 ) &\n",
      "69.82 \\% ( 0.24 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "75.66 \\% ( 0.08 ) &\n",
      "96.55 \\% ( 0.07 ) &\n",
      "66.63 \\% ( 0.22 ) &\n",
      "66.48 \\% ( 0.05 ) &\n",
      "54.98 \\% ( 0.24 ) &\n",
      "56.23 \\% ( 0.34 ) &\n",
      "70.67 \\% ( 0.1 ) &\n",
      "92.2 \\% ( 0.07 ) &\n",
      "68.29 \\% ( 0.33 ) &\n",
      "Latex Recalls\n",
      "82.61 \\% ( 0.16 ) &\n",
      "56.68 \\% ( 0.2 ) &\n",
      "26.49 \\% ( 0.09 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "76.88 \\% ( 0.09 ) &\n",
      "99.98 \\% ( 0.0 ) &\n",
      "12.67 \\% ( 0.09 ) &\n",
      "75.7 \\% ( 0.1 ) &\n",
      "29.2 \\% ( 0.2 ) &\n",
      "24.0 \\% ( 0.21 ) &\n",
      "13.93 \\% ( 0.02 ) &\n",
      "78.02 \\% ( 0.03 ) &\n",
      "16.53 \\% ( 0.13 ) &\n"
     ]
    }
   ],
   "source": [
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2897dbd25f4012b540a556245633ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.8006090080302325\n",
      "0 Precision std= 0.04380101633269852\n",
      "1 Precision mean= 0.778464254853118\n",
      "1 Precision std= 0.1694008158871531\n",
      "2 Precision mean= 0.5282633338775911\n",
      "2 Precision std= 0.18311822541573566\n",
      "3 Precision mean= 1.0\n",
      "3 Precision std= 0.0\n",
      "4 Precision mean= 0.9860622662715086\n",
      "4 Precision std= 0.028052077490259553\n",
      "5 Precision mean= 1.0\n",
      "5 Precision std= 0.0\n",
      "6 Precision mean= 0.6342802369686409\n",
      "6 Precision std= 0.19945133766872472\n",
      "7 Precision mean= 0.7226401178321804\n",
      "7 Precision std= 0.07771506358806067\n",
      "8 Precision mean= 0.6185142578486207\n",
      "8 Precision std= 0.23131752682581408\n",
      "9 Precision mean= 0.4779468849331163\n",
      "9 Precision std= 0.2370162026242056\n",
      "10 Precision mean= 0.6951328855966932\n",
      "10 Precision std= 0.09012678963310154\n",
      "11 Precision mean= 0.9508998894343581\n",
      "11 Precision std= 0.04766869432546471\n",
      "12 Precision mean= 0.439416478398252\n",
      "12 Precision std= 0.21582984116456264\n",
      "Recall mean of  0  = 0.9727090870428023\n",
      "Recall std of  0  = 0.02045218508577791\n",
      "Recall mean of  1  = 0.6253728098945655\n",
      "Recall std of  1  = 0.16833079771124654\n",
      "Recall mean of  2  = 0.2998865137917399\n",
      "Recall std of  2  = 0.12256569180974208\n",
      "Recall mean of  3  = 0.9995989261831276\n",
      "Recall std of  3  = 0.0012036750545582845\n",
      "Recall mean of  4  = 0.8299968382396841\n",
      "Recall std of  4  = 0.12238656949153728\n",
      "Recall mean of  5  = 0.9990186527574242\n",
      "Recall std of  5  = 0.0017017239831954515\n",
      "Recall mean of  6  = 0.15261945599100815\n",
      "Recall std of  6  = 0.08051527373432954\n",
      "Recall mean of  7  = 0.8722033474546039\n",
      "Recall std of  7  = 0.17315511846998885\n",
      "Recall mean of  8  = 0.35952653991331457\n",
      "Recall std of  8  = 0.20653876305130806\n",
      "Recall mean of  9  = 0.3587135520495091\n",
      "Recall std of  9  = 0.19347072318809105\n",
      "Recall mean of  10  = 0.14014557711511783\n",
      "Recall std of  10  = 0.03344902128670585\n",
      "Recall mean of  11  = 0.7852069451974335\n",
      "Recall std of  11  = 0.035070445217887006\n",
      "Recall mean of  12  = 0.2395540498875885\n",
      "Recall std of  12  = 0.15023396957311067\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = rs)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = accs\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = recalls\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex Precisions\n",
      "80.06 \\% ( 0.04 ) &\n",
      "77.85 \\% ( 0.17 ) &\n",
      "52.83 \\% ( 0.18 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "98.61 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "63.43 \\% ( 0.2 ) &\n",
      "72.26 \\% ( 0.08 ) &\n",
      "61.85 \\% ( 0.23 ) &\n",
      "47.79 \\% ( 0.24 ) &\n",
      "69.51 \\% ( 0.09 ) &\n",
      "95.09 \\% ( 0.05 ) &\n",
      "43.94 \\% ( 0.22 ) &\n",
      "Latex Recalls\n",
      "97.27 \\% ( 0.02 ) &\n",
      "62.54 \\% ( 0.17 ) &\n",
      "29.99 \\% ( 0.12 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "83.0 \\% ( 0.12 ) &\n",
      "99.9 \\% ( 0.0 ) &\n",
      "15.26 \\% ( 0.08 ) &\n",
      "87.22 \\% ( 0.17 ) &\n",
      "35.95 \\% ( 0.21 ) &\n",
      "35.87 \\% ( 0.19 ) &\n",
      "14.01 \\% ( 0.03 ) &\n",
      "78.52 \\% ( 0.04 ) &\n",
      "23.96 \\% ( 0.15 ) &\n"
     ]
    }
   ],
   "source": [
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937cbee9d019435d91c8c0d6709c812f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.6972837585359323\n",
      "0 Precision std= 0.018303512465238474\n",
      "1 Precision mean= 0.4038418216092398\n",
      "1 Precision std= 0.03937783178100957\n",
      "2 Precision mean= 0.527915416028818\n",
      "2 Precision std= 0.04127416591604016\n",
      "3 Precision mean= 0.9986146263616664\n",
      "3 Precision std= 0.002886313340697261\n",
      "4 Precision mean= 0.7418605439876964\n",
      "4 Precision std= 0.06688044980148687\n",
      "5 Precision mean= 0.9993941852425012\n",
      "5 Precision std= 0.001442356923587901\n",
      "6 Precision mean= 0.8206673433801329\n",
      "6 Precision std= 0.14557647066506124\n",
      "7 Precision mean= 0.6777563878212962\n",
      "7 Precision std= 0.06208623882638545\n",
      "8 Precision mean= 0.3309501884819273\n",
      "8 Precision std= 0.05465581264530748\n",
      "9 Precision mean= 0.49680769876716424\n",
      "9 Precision std= 0.0770678917169437\n",
      "10 Precision mean= 0.6444430145872899\n",
      "10 Precision std= 0.1020292679184349\n",
      "11 Precision mean= 0.8785379113370929\n",
      "11 Precision std= 0.01608047448712708\n",
      "12 Precision mean= 0.0\n",
      "12 Precision std= 0.0\n",
      "Recall mean of  0  = 0.9908181767574199\n",
      "Recall std of  0  = 0.0017703357947003071\n",
      "Recall mean of  1  = 0.3016254061064673\n",
      "Recall std of  1  = 0.041211656196641\n",
      "Recall mean of  2  = 0.257612225628053\n",
      "Recall std of  2  = 0.0265342020761714\n",
      "Recall mean of  3  = 0.996808585984108\n",
      "Recall std of  3  = 0.004131233872392726\n",
      "Recall mean of  4  = 0.7542956926455091\n",
      "Recall std of  4  = 0.08943365833823023\n",
      "Recall mean of  5  = 0.9818004593637101\n",
      "Recall std of  5  = 0.008751467899456557\n",
      "Recall mean of  6  = 0.06616519043239974\n",
      "Recall std of  6  = 0.019907773303973083\n",
      "Recall mean of  7  = 0.7054663218589996\n",
      "Recall std of  7  = 0.10268883715087744\n",
      "Recall mean of  8  = 0.1342110167954737\n",
      "Recall std of  8  = 0.027855741946911896\n",
      "Recall mean of  9  = 0.0931470034878791\n",
      "Recall std of  9  = 0.017097951857740445\n",
      "Recall mean of  10  = 0.06495529219292767\n",
      "Recall std of  10  = 0.015933903753571288\n",
      "Recall mean of  11  = 0.7438119710193607\n",
      "Recall std of  11  = 0.032016280750955056\n",
      "Recall mean of  12  = 0.0\n",
      "Recall std of  12  = 0.0\n",
      "Latex Precisions\n",
      "69.73 \\% ( 0.02 ) &\n",
      "40.38 \\% ( 0.04 ) &\n",
      "52.79 \\% ( 0.04 ) &\n",
      "99.86 \\% ( 0.0 ) &\n",
      "74.19 \\% ( 0.07 ) &\n",
      "99.94 \\% ( 0.0 ) &\n",
      "82.07 \\% ( 0.15 ) &\n",
      "67.78 \\% ( 0.06 ) &\n",
      "33.1 \\% ( 0.05 ) &\n",
      "49.68 \\% ( 0.08 ) &\n",
      "64.44 \\% ( 0.1 ) &\n",
      "87.85 \\% ( 0.02 ) &\n",
      "0.0 \\% ( 0.0 ) &\n",
      "Latex Recalls\n",
      "99.08 \\% ( 0.0 ) &\n",
      "30.16 \\% ( 0.04 ) &\n",
      "25.76 \\% ( 0.03 ) &\n",
      "99.68 \\% ( 0.0 ) &\n",
      "75.43 \\% ( 0.09 ) &\n",
      "98.18 \\% ( 0.01 ) &\n",
      "6.62 \\% ( 0.02 ) &\n",
      "70.55 \\% ( 0.1 ) &\n",
      "13.42 \\% ( 0.03 ) &\n",
      "9.31 \\% ( 0.02 ) &\n",
      "6.5 \\% ( 0.02 ) &\n",
      "74.38 \\% ( 0.03 ) &\n",
      "0.0 \\% ( 0.0 ) &\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = LogisticRegression(random_state = epoch)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1edf77c3b884eba900a4edf088326a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7480887457555241\n",
      "0 Precision std= 0.016593364965375768\n",
      "1 Precision mean= 0.5419835949755898\n",
      "1 Precision std= 0.028222316490708946\n",
      "2 Precision mean= 0.48929620907957927\n",
      "2 Precision std= 0.03691055437913015\n",
      "3 Precision mean= 0.9962746197645146\n",
      "3 Precision std= 0.0035993217922279234\n",
      "4 Precision mean= 0.7095183709391046\n",
      "4 Precision std= 0.02224567467586668\n",
      "5 Precision mean= 0.9980016885928794\n",
      "5 Precision std= 0.0032779936852438754\n",
      "6 Precision mean= 0.33120715558394476\n",
      "6 Precision std= 0.07247355892328458\n",
      "7 Precision mean= 0.7035021999864176\n",
      "7 Precision std= 0.027260892697049437\n",
      "8 Precision mean= 0.687061573274929\n",
      "8 Precision std= 0.03701352708301233\n",
      "9 Precision mean= 0.4018288689149343\n",
      "9 Precision std= 0.05541492568783841\n",
      "10 Precision mean= 0.38269939622721905\n",
      "10 Precision std= 0.04870929614910959\n",
      "11 Precision mean= 0.8470160659294919\n",
      "11 Precision std= 0.022105486316875343\n",
      "12 Precision mean= 0.20763897919367\n",
      "12 Precision std= 0.05495894102530598\n",
      "Recall mean of  0  = 0.9280691832872051\n",
      "Recall std of  0  = 0.007058699656318592\n",
      "Recall mean of  1  = 0.7992558856806952\n",
      "Recall std of  1  = 0.028393077258956147\n",
      "Recall mean of  2  = 0.3044100307612331\n",
      "Recall std of  2  = 0.02768506884606984\n",
      "Recall mean of  3  = 0.9990066285834525\n",
      "Recall std of  3  = 0.0017220456921451172\n",
      "Recall mean of  4  = 0.7940252370081601\n",
      "Recall std of  4  = 0.031467300628542874\n",
      "Recall mean of  5  = 0.9942176529294671\n",
      "Recall std of  5  = 0.004410811107993635\n",
      "Recall mean of  6  = 0.09912380311937018\n",
      "Recall std of  6  = 0.019842336749312506\n",
      "Recall mean of  7  = 0.6231735483325813\n",
      "Recall std of  7  = 0.03477458399719955\n",
      "Recall mean of  8  = 0.6566896438268433\n",
      "Recall std of  8  = 0.02754998361540661\n",
      "Recall mean of  9  = 0.12796721877699255\n",
      "Recall std of  9  = 0.018716789830501304\n",
      "Recall mean of  10  = 0.13475606641954369\n",
      "Recall std of  10  = 0.015647226646880383\n",
      "Recall mean of  11  = 0.7507963497045222\n",
      "Recall std of  11  = 0.029387809538135173\n",
      "Recall mean of  12  = 0.03835848954510491\n",
      "Recall std of  12  = 0.011934061302023258\n",
      "Latex Precisions\n",
      "74.81 \\% ( 0.02 ) &\n",
      "54.2 \\% ( 0.03 ) &\n",
      "48.93 \\% ( 0.04 ) &\n",
      "99.63 \\% ( 0.0 ) &\n",
      "70.95 \\% ( 0.02 ) &\n",
      "99.8 \\% ( 0.0 ) &\n",
      "33.12 \\% ( 0.07 ) &\n",
      "70.35 \\% ( 0.03 ) &\n",
      "68.71 \\% ( 0.04 ) &\n",
      "40.18 \\% ( 0.06 ) &\n",
      "38.27 \\% ( 0.05 ) &\n",
      "84.7 \\% ( 0.02 ) &\n",
      "20.76 \\% ( 0.05 ) &\n",
      "Latex Recalls\n",
      "92.81 \\% ( 0.01 ) &\n",
      "79.93 \\% ( 0.03 ) &\n",
      "30.44 \\% ( 0.03 ) &\n",
      "99.9 \\% ( 0.0 ) &\n",
      "79.4 \\% ( 0.03 ) &\n",
      "99.42 \\% ( 0.0 ) &\n",
      "9.91 \\% ( 0.02 ) &\n",
      "62.32 \\% ( 0.03 ) &\n",
      "65.67 \\% ( 0.03 ) &\n",
      "12.8 \\% ( 0.02 ) &\n",
      "13.48 \\% ( 0.02 ) &\n",
      "75.08 \\% ( 0.03 ) &\n",
      "3.84 \\% ( 0.01 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be479b79c7a4840bacae0fe7656acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.90589853888143\n",
      "0 Precision std= 0.006580613078238353\n",
      "1 Precision mean= 0.9366169078043483\n",
      "1 Precision std= 0.014485752411366626\n",
      "2 Precision mean= 0.6415434786173213\n",
      "2 Precision std= 0.02361746883293334\n",
      "3 Precision mean= 0.9995997694672131\n",
      "3 Precision std= 0.0012010757582163777\n",
      "4 Precision mean= 0.9997863247863247\n",
      "4 Precision std= 0.0009313886631497133\n",
      "5 Precision mean= 0.9985836138618666\n",
      "5 Precision std= 0.0030048836765764414\n",
      "6 Precision mean= 0.8113134230011194\n",
      "6 Precision std= 0.02879803513308709\n",
      "7 Precision mean= 0.8307061444642816\n",
      "7 Precision std= 0.021755638790884855\n",
      "8 Precision mean= 0.8064086597578095\n",
      "8 Precision std= 0.03755852021785365\n",
      "9 Precision mean= 0.7592343586622707\n",
      "9 Precision std= 0.034898868632500524\n",
      "10 Precision mean= 0.6729387806635958\n",
      "10 Precision std= 0.025266515705357595\n",
      "11 Precision mean= 0.9636122397297683\n",
      "11 Precision std= 0.010653571517084418\n",
      "12 Precision mean= 0.8107260470177945\n",
      "12 Precision std= 0.04936101341480204\n",
      "Recall mean of  0  = 0.9636885952216276\n",
      "Recall std of  0  = 0.0035420455182158714\n",
      "Recall mean of  1  = 0.8948090876283402\n",
      "Recall std of  1  = 0.01439842835802929\n",
      "Recall mean of  2  = 0.6227273876455299\n",
      "Recall std of  2  = 0.04357497818069272\n",
      "Recall mean of  3  = 0.9995950354958765\n",
      "Recall std of  3  = 0.001215070507024462\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n",
      "Recall mean of  5  = 0.9993957311019575\n",
      "Recall std of  5  = 0.0014387841144611298\n",
      "Recall mean of  6  = 0.640199203963274\n",
      "Recall std of  6  = 0.021887937176097536\n",
      "Recall mean of  7  = 0.928769021453841\n",
      "Recall std of  7  = 0.027305688790368314\n",
      "Recall mean of  8  = 0.8370079270019595\n",
      "Recall std of  8  = 0.03448589909505018\n",
      "Recall mean of  9  = 0.6697863814497019\n",
      "Recall std of  9  = 0.03556815334583653\n",
      "Recall mean of  10  = 0.5035863702099268\n",
      "Recall std of  10  = 0.028149844368548546\n",
      "Recall mean of  11  = 0.8920696903962174\n",
      "Recall std of  11  = 0.02289417528921394\n",
      "Recall mean of  12  = 0.6444093277275696\n",
      "Recall std of  12  = 0.02349175744183058\n",
      "Latex Precisions\n",
      "90.59 \\% ( 0.01 ) &\n",
      "93.66 \\% ( 0.01 ) &\n",
      "64.15 \\% ( 0.02 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "99.98 \\% ( 0.0 ) &\n",
      "99.86 \\% ( 0.0 ) &\n",
      "81.13 \\% ( 0.03 ) &\n",
      "83.07 \\% ( 0.02 ) &\n",
      "80.64 \\% ( 0.04 ) &\n",
      "75.92 \\% ( 0.03 ) &\n",
      "67.29 \\% ( 0.03 ) &\n",
      "96.36 \\% ( 0.01 ) &\n",
      "81.07 \\% ( 0.05 ) &\n",
      "Latex Recalls\n",
      "96.37 \\% ( 0.0 ) &\n",
      "89.48 \\% ( 0.01 ) &\n",
      "62.27 \\% ( 0.04 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "99.94 \\% ( 0.0 ) &\n",
      "64.02 \\% ( 0.02 ) &\n",
      "92.88 \\% ( 0.03 ) &\n",
      "83.7 \\% ( 0.03 ) &\n",
      "66.98 \\% ( 0.04 ) &\n",
      "50.36 \\% ( 0.03 ) &\n",
      "89.21 \\% ( 0.02 ) &\n",
      "64.44 \\% ( 0.02 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = rs + i)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2b3b057d41442bb4a56de94b4c0b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7018560977899171\n",
      "0 Precision std= 0.01823249266520187\n",
      "1 Precision mean= 0.4211066053685132\n",
      "1 Precision std= 0.0765853174386387\n",
      "2 Precision mean= 0.4711404864775954\n",
      "2 Precision std= 0.04005645546023005\n",
      "3 Precision mean= 0.9966277564913211\n",
      "3 Precision std= 0.003125778608181801\n",
      "4 Precision mean= 0.9524019413331141\n",
      "4 Precision std= 0.10602799176390985\n",
      "5 Precision mean= 0.8435448069392588\n",
      "5 Precision std= 0.054839739133382644\n",
      "6 Precision mean= 0.9898793363499246\n",
      "6 Precision std= 0.024262373302972276\n",
      "7 Precision mean= 0.5981090682805664\n",
      "7 Precision std= 0.03677171817651272\n",
      "8 Precision mean= 0.4164394749442808\n",
      "8 Precision std= 0.10324497402064176\n",
      "9 Precision mean= 0.8020319943349445\n",
      "9 Precision std= 0.1013041515008211\n",
      "10 Precision mean= 0.7064000182877034\n",
      "10 Precision std= 0.06939506059932997\n",
      "11 Precision mean= 0.8770294682065561\n",
      "11 Precision std= 0.015897778270731723\n",
      "12 Precision mean= 0.0\n",
      "12 Precision std= 0.0\n",
      "Recall mean of  0  = 0.9866006485198149\n",
      "Recall std of  0  = 0.002320196950575675\n",
      "Recall mean of  1  = 0.28408196019603116\n",
      "Recall std of  1  = 0.030519732827806217\n",
      "Recall mean of  2  = 0.2580243347423014\n",
      "Recall std of  2  = 0.026208281463225964\n",
      "Recall mean of  3  = 0.9933819444139977\n",
      "Recall std of  3  = 0.00662569186159084\n",
      "Recall mean of  4  = 0.5954007077241281\n",
      "Recall std of  4  = 0.08461242824500141\n",
      "Recall mean of  5  = 0.9960707512049524\n",
      "Recall std of  5  = 0.004508308477940797\n",
      "Recall mean of  6  = 0.047779430065307396\n",
      "Recall std of  6  = 0.016476639739395556\n",
      "Recall mean of  7  = 0.8844025883404452\n",
      "Recall std of  7  = 0.08802232365381094\n",
      "Recall mean of  8  = 0.1846895301446546\n",
      "Recall std of  8  = 0.08550658472809052\n",
      "Recall mean of  9  = 0.0931470034878791\n",
      "Recall std of  9  = 0.017097951857740445\n",
      "Recall mean of  10  = 0.09832010717285841\n",
      "Recall std of  10  = 0.014088050086836945\n",
      "Recall mean of  11  = 0.7322454651332235\n",
      "Recall std of  11  = 0.03244708705080332\n",
      "Recall mean of  12  = 0.0\n",
      "Recall std of  12  = 0.0\n",
      "Latex Precisions\n",
      "70.19 \\% ( 0.02 ) &\n",
      "42.11 \\% ( 0.08 ) &\n",
      "47.11 \\% ( 0.04 ) &\n",
      "99.66 \\% ( 0.0 ) &\n",
      "95.24 \\% ( 0.11 ) &\n",
      "84.35 \\% ( 0.05 ) &\n",
      "98.99 \\% ( 0.02 ) &\n",
      "59.81 \\% ( 0.04 ) &\n",
      "41.64 \\% ( 0.1 ) &\n",
      "80.2 \\% ( 0.1 ) &\n",
      "70.64 \\% ( 0.07 ) &\n",
      "87.7 \\% ( 0.02 ) &\n",
      "0.0 \\% ( 0.0 ) &\n",
      "Latex Recalls\n",
      "98.66 \\% ( 0.0 ) &\n",
      "28.41 \\% ( 0.03 ) &\n",
      "25.8 \\% ( 0.03 ) &\n",
      "99.34 \\% ( 0.01 ) &\n",
      "59.54 \\% ( 0.08 ) &\n",
      "99.61 \\% ( 0.0 ) &\n",
      "4.78 \\% ( 0.02 ) &\n",
      "88.44 \\% ( 0.09 ) &\n",
      "18.47 \\% ( 0.09 ) &\n",
      "9.31 \\% ( 0.02 ) &\n",
      "9.83 \\% ( 0.01 ) &\n",
      "73.22 \\% ( 0.03 ) &\n",
      "0.0 \\% ( 0.0 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad6a9ebe595c8604775664ccb6b4edb43e0871577ed89c469f16f6efb723bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
