{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the repeat experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = [\"Backdoor.csv\" , \"DDoS_HTTP.csv\" , \"DDoS_ICMP.csv\" , \"DDoS_TCP.csv\" , \"DDoS_UDP.csv\"  , \"Password.csv\" , \"Port_Scanning.csv\" ,\"Ransomware.csv\", \"SQL_injection.csv\" , \"Uploading.csv\" , \"Vulnerability_scanner.csv\" ,\"XSS.csv\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, clfs, p_nodes, gms):\n",
    "    p_x_node = np.zeros((len(x), len(clfs))) # p[x, in node i] = p(x | x in Node i) * p(x in Node i)\n",
    "    p_y_given_node = np.zeros((len(x), len(clfs))) # p[y | x, node i]\n",
    "    for i in range(len(clfs)):\n",
    "        p_x_node[:, i] = p_nodes[i] * np.exp(gms[i].score_samples(x))\n",
    "        p_y_given_node[:, i] = clfs[i].predict_proba(x)[:, 1]\n",
    "    \n",
    "    p_y = p_y_given_node * p_x_node / (np.sum(p_x_node, axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "    p_y = np.hstack([1-np.sum(p_y, axis=-1, keepdims=True), p_y])\n",
    "\n",
    "    return np.argmax(p_y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(input_file)):\n",
    "    data = pd.read_csv(input_file[i])\n",
    "    data['attack_type'] = i + 1\n",
    "    data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "    data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "    if len(data) < 2000:\n",
    "        data_list.append(data)\n",
    "    else:\n",
    "        data0 = data[data['attack_type'].isin([0])]\n",
    "        x1 = data0.sample(n = 1000, random_state = i + 42, axis = 0)\n",
    "        data1 = data[data['attack_type'].isin([i+1])]\n",
    "        x2 = data1.sample(n = 1000, random_state = i + 43, axis = 0)   \n",
    "        data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "        data_list.append(data_selected) \n",
    "for i in range(len(data_list)):\n",
    "    print(len(data_list[i]))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>http.content_length</th>\n",
       "      <th>http.response</th>\n",
       "      <th>tcp.ack</th>\n",
       "      <th>tcp.ack_raw</th>\n",
       "      <th>tcp.checksum</th>\n",
       "      <th>tcp.connection.fin</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.conflag.cleansess</th>\n",
       "      <th>mqtt.conflags</th>\n",
       "      <th>mqtt.hdrflags</th>\n",
       "      <th>mqtt.len</th>\n",
       "      <th>mqtt.msgtype</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1934832499</td>\n",
       "      <td>19721</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24507</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3703765364</td>\n",
       "      <td>46928</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>208620284</td>\n",
       "      <td>44688</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3557485597</td>\n",
       "      <td>42863</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      arp.opcode  arp.hw.size  icmp.checksum  icmp.seq_le  \\\n",
       "1995           0            0              0            0   \n",
       "1996           0            0              0            0   \n",
       "1997           0            0              0            0   \n",
       "1998           0            0              0            0   \n",
       "1999           0            0              0            0   \n",
       "\n",
       "      http.content_length  http.response  tcp.ack  tcp.ack_raw  tcp.checksum  \\\n",
       "1995                    0              0      150   1934832499         19721   \n",
       "1996                    0              0        0            0         24507   \n",
       "1997                    0              0        1   3703765364         46928   \n",
       "1998                    0              0        1    208620284         44688   \n",
       "1999                    0              0        1   3557485597         42863   \n",
       "\n",
       "      tcp.connection.fin  ...  mqtt.conflag.cleansess  mqtt.conflags  \\\n",
       "1995                   0  ...                       0              0   \n",
       "1996                   0  ...                       0              0   \n",
       "1997                   0  ...                       0              0   \n",
       "1998                   0  ...                       0              0   \n",
       "1999                   0  ...                       0              0   \n",
       "\n",
       "      mqtt.hdrflags  mqtt.len  mqtt.msgtype  mqtt.proto_len  mqtt.topic_len  \\\n",
       "1995              0         0             0               0               0   \n",
       "1996              0         0             0               0               0   \n",
       "1997              0         0             0               0               0   \n",
       "1998              0         0             0               0               0   \n",
       "1999              0         0             0               0               0   \n",
       "\n",
       "      mqtt.ver  Attack_label  attack_type  \n",
       "1995         0             1           10  \n",
       "1996         0             1           10  \n",
       "1997         0             1           10  \n",
       "1998         0             1           10  \n",
       "1999         0             1           10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[9].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42 # set the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2857d2143f0c4572a9f515d4aba44b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8474217e03a4a8999934e275de161b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total number is  24000\n",
      "The proportion of the nodes are: [0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      " 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clfs = [] # This is going to contain 14 different classifiers\n",
    "n_samples = []\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "for i in tqdm(range(len(input_file))): # reading the data\n",
    "    data0 = data_list[i]\n",
    "    y[i][data0.attack_type == 0] = 0\n",
    "    n_samples.append(len(y[i])) # the number of this node\n",
    "    x = data_list[i]\n",
    "    x.drop(columns='attack_type')\n",
    "    x.drop(columns='Attack_label')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = rs)\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_test_list.append(x_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    #classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "    # classifier = SVC(kernel = 'rbf', random_state = rs, gamma='scale', max_iter=-1, probability=True)\n",
    "    # classifier.fit(data_list[i], y[i])\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(x_train_list[i], y_train_list[i])\n",
    "    clfs.append(classifier)\n",
    "\n",
    "total_n_samples = np.sum(n_samples)\n",
    "print('total number is ',total_n_samples)\n",
    "p_nodes =  np.array(n_samples) / total_n_samples\n",
    "print('The proportion of the nodes are:', p_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_file)):\n",
    "    x_test_list[i] = scaler.transform(x_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.vstack(x_test_list)\n",
    "y_test = np.hstack(y_test_list)\n",
    "x_train = np.vstack(x_train_list)\n",
    "y_train = np.hstack(y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db12767fc3948b084324523f25489f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gms = []\n",
    "K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    x = x_train_list[i]\n",
    "    gm = GaussianMixture(n_components = K).fit(x)  \n",
    "    gms.append(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction = pred(x_test, clfs, p_nodes, gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is: 0.9991666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = prediction == y_test\n",
    "accuracy = np.mean(correct)\n",
    "print('Overall accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [1.0, 1.0, 0.9958847736625515, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9876543209876543, 0.9958847736625515]\n",
      "Recalls: [0.9983813531887342, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[y_test==i]) == 0:\n",
    "        accs.append(0)\n",
    "    else:\n",
    "        accs.append(np.mean(correct[y_test==i]))\n",
    "print('Accuracies:', accs)\n",
    "recalls = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[prediction==i]) == 0:\n",
    "        recalls.append(0)\n",
    "    else:\n",
    "        recalls.append(np.mean(correct[prediction==i]))\n",
    "print('Recalls:', recalls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e52d38877440c291d516d3f3b38cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.9987491367325021\n",
      "0 Precision std= 0.0007168110173182934\n",
      "1 Precision mean= 0.9980049114476872\n",
      "1 Precision std= 0.00456138534379315\n",
      "2 Precision mean= 1.0\n",
      "2 Precision std= 0.0\n",
      "3 Precision mean= 1.0\n",
      "3 Precision std= 0.0\n",
      "4 Precision mean= 1.0\n",
      "4 Precision std= 0.0\n",
      "5 Precision mean= 0.9942352697017842\n",
      "5 Precision std= 0.014049899407270689\n",
      "6 Precision mean= 1.0\n",
      "6 Precision std= 0.0\n",
      "7 Precision mean= 1.0\n",
      "7 Precision std= 0.0\n",
      "8 Precision mean= 0.9998000000000001\n",
      "8 Precision std= 0.0008717797887081355\n",
      "9 Precision mean= 1.0\n",
      "9 Precision std= 0.0\n",
      "10 Precision mean= 1.0\n",
      "10 Precision std= 0.0\n",
      "11 Precision mean= 1.0\n",
      "11 Precision std= 0.0\n",
      "12 Precision mean= 1.0\n",
      "12 Precision std= 0.0\n",
      "Recall mean of  0  = 0.9993186073759459\n",
      "Recall std of  0  = 0.0014446323326059612\n",
      "Recall mean of  1  = 0.9998154981549815\n",
      "Recall std of  1  = 0.0008042248973322168\n",
      "Recall mean of  2  = 0.9961963776448831\n",
      "Recall std of  2  = 0.004762947398046391\n",
      "Recall mean of  3  = 1.0\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n",
      "Recall mean of  5  = 1.0\n",
      "Recall std of  5  = 0.0\n",
      "Recall mean of  6  = 0.9969918015969066\n",
      "Recall std of  6  = 0.003333148092595597\n",
      "Recall mean of  7  = 1.0\n",
      "Recall std of  7  = 0.0\n",
      "Recall mean of  8  = 0.9998046875\n",
      "Recall std of  8  = 0.0008513474499102879\n",
      "Recall mean of  9  = 0.9993893538376231\n",
      "Recall std of  9  = 0.0014537161058307195\n",
      "Recall mean of  10  = 0.99939649703333\n",
      "Recall std of  10  = 0.0014370209554917348\n",
      "Recall mean of  11  = 0.9955844781505672\n",
      "Recall std of  11  = 0.005284759368114378\n",
      "Recall mean of  12  = 0.9978175944569754\n",
      "Recall std of  12  = 0.0029615403016962382\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x.drop(columns='attack_type')\n",
    "        x.drop(columns='Attack_label')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        #classifier = LogisticRegression(random_state=epoch)\n",
    "        classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 15 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = accs\n",
    "      \n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = recalls\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c750b19768a43a9a13081ceb86f1c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 1.0\n",
      "0 Precision std= 0.0\n",
      "1 Precision mean= 0.990442188321613\n",
      "1 Precision std= 0.009441524909574115\n",
      "2 Precision mean= 0.993199725013859\n",
      "2 Precision std= 0.00652531463590949\n",
      "3 Precision mean= 0.9986139223544572\n",
      "3 Precision std= 0.0028868905337346294\n",
      "4 Precision mean= 0.9967121489684914\n",
      "4 Precision std= 0.004257425642298421\n",
      "5 Precision mean= 0.9982319551681963\n",
      "5 Precision std= 0.0038521438365888444\n",
      "6 Precision mean= 0.988618252823995\n",
      "6 Precision std= 0.009381431594957931\n",
      "7 Precision mean= 0.9093605023900228\n",
      "7 Precision std= 0.02175382086758046\n",
      "8 Precision mean= 0.8819766258653983\n",
      "8 Precision std= 0.018838060572445543\n",
      "9 Precision mean= 0.9528630738500702\n",
      "9 Precision std= 0.034568072834674876\n",
      "10 Precision mean= 0.8808136183445214\n",
      "10 Precision std= 0.031247136662436076\n",
      "11 Precision mean= 0.9350358907289085\n",
      "11 Precision std= 0.024998532463082656\n",
      "12 Precision mean= 0.9046096937180306\n",
      "12 Precision std= 0.02751995038132959\n",
      "Recall mean of  0  = 0.9999670619235836\n",
      "Recall std of  0  = 0.00014357374649344182\n",
      "Recall mean of  1  = 0.9950843809811827\n",
      "Recall std of  1  = 0.005878255296027442\n",
      "Recall mean of  2  = 0.9902953348028228\n",
      "Recall std of  2  = 0.009663351472692238\n",
      "Recall mean of  3  = 0.9968212220071798\n",
      "Recall std of  3  = 0.004271697939686913\n",
      "Recall mean of  4  = 0.9945774557703363\n",
      "Recall std of  4  = 0.004258668727830598\n",
      "Recall mean of  5  = 0.9986072730844431\n",
      "Recall std of  5  = 0.002913019447836434\n",
      "Recall mean of  6  = 0.9921152276125287\n",
      "Recall std of  6  = 0.006256824009403261\n",
      "Recall mean of  7  = 0.9532197724857661\n",
      "Recall std of  7  = 0.017674403602856895\n",
      "Recall mean of  8  = 0.9001141410839338\n",
      "Recall std of  8  = 0.0363281961082816\n",
      "Recall mean of  9  = 0.8438659523708028\n",
      "Recall std of  9  = 0.04438445783981835\n",
      "Recall mean of  10  = 0.9380911065594386\n",
      "Recall std of  10  = 0.029065070243060712\n",
      "Recall mean of  11  = 0.8281517734654786\n",
      "Recall std of  11  = 0.02513930557754542\n",
      "Recall mean of  12  = 0.984224434705855\n",
      "Recall std of  12  = 0.013841131534062879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x.drop(columns='attack_type')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = LogisticRegression(random_state = epoch)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM_Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba20248178064d39885a7db0a2945b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.999984399375975\n",
      "0 Precision std= 6.800154358096656e-05\n",
      "1 Precision mean= 0.9976529648724457\n",
      "1 Precision std= 0.0031631506966908443\n",
      "2 Precision mean= 0.965556772460076\n",
      "2 Precision std= 0.007356644844461791\n",
      "3 Precision mean= 0.9972321211924602\n",
      "3 Precision std= 0.002825281657089061\n",
      "4 Precision mean= 0.9912792684879147\n",
      "4 Precision std= 0.009627056439872949\n",
      "5 Precision mean= 0.9895729633941311\n",
      "5 Precision std= 0.011034833193231203\n",
      "6 Precision mean= 0.9682193232693311\n",
      "6 Precision std= 0.010985600822172432\n",
      "7 Precision mean= 0.9004940224928726\n",
      "7 Precision std= 0.026367215887007512\n",
      "8 Precision mean= 0.9453368874215655\n",
      "8 Precision std= 0.020426057223423884\n",
      "9 Precision mean= 0.8620218018595684\n",
      "9 Precision std= 0.03530138198903517\n",
      "10 Precision mean= 0.817008731203266\n",
      "10 Precision std= 0.024687076343400154\n",
      "11 Precision mean= 0.888968799300286\n",
      "11 Precision std= 0.026869232499324158\n",
      "12 Precision mean= 0.9471486421612425\n",
      "12 Precision std= 0.02471964580277541\n",
      "Recall mean of  0  = 0.9999169036369462\n",
      "Recall std of  0  = 0.00014403454896402834\n",
      "Recall mean of  1  = 0.9407747321598912\n",
      "Recall std of  1  = 0.01586193853857397\n",
      "Recall mean of  2  = 0.9981820484378412\n",
      "Recall std of  2  = 0.0026777607018584563\n",
      "Recall mean of  3  = 0.9960075193187654\n",
      "Recall std of  3  = 0.005658210603059454\n",
      "Recall mean of  4  = 0.9978082874510589\n",
      "Recall std of  4  = 0.003213064477575078\n",
      "Recall mean of  5  = 0.9972304317868872\n",
      "Recall std of  5  = 0.002831263136374642\n",
      "Recall mean of  6  = 0.9983991115588134\n",
      "Recall std of  6  = 0.002316942769305635\n",
      "Recall mean of  7  = 0.9255279161912489\n",
      "Recall std of  7  = 0.025856026579940992\n",
      "Recall mean of  8  = 0.8529348548180076\n",
      "Recall std of  8  = 0.023148759936307623\n",
      "Recall mean of  9  = 0.8987989724326864\n",
      "Recall std of  9  = 0.019484387403057614\n",
      "Recall mean of  10  = 0.8908439373128798\n",
      "Recall std of  10  = 0.03650449568074221\n",
      "Recall mean of  11  = 0.8269466457744274\n",
      "Recall std of  11  = 0.025779074463737332\n",
      "Recall mean of  12  = 0.9291654680905813\n",
      "Recall std of  12  = 0.01364648903119385\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x.drop(columns='attack_type')\n",
    "        x.drop(columns='Attack_label')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = SVC(kernel = 'rbf', gamma='scale',max_iter=-1, probability=True)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7254e9e1789a4199ba398feb82533ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 1.0\n",
      "0 Precision std= 0.0\n",
      "1 Precision mean= 0.990442188321613\n",
      "1 Precision std= 0.009441524909574115\n",
      "2 Precision mean= 0.993199725013859\n",
      "2 Precision std= 0.00652531463590949\n",
      "3 Precision mean= 0.9986139223544572\n",
      "3 Precision std= 0.0028868905337346294\n",
      "4 Precision mean= 0.9967121489684914\n",
      "4 Precision std= 0.004257425642298421\n",
      "5 Precision mean= 0.9982319551681963\n",
      "5 Precision std= 0.0038521438365888444\n",
      "6 Precision mean= 0.988618252823995\n",
      "6 Precision std= 0.009381431594957931\n",
      "7 Precision mean= 0.9093605023900228\n",
      "7 Precision std= 0.02175382086758046\n",
      "8 Precision mean= 0.8819766258653983\n",
      "8 Precision std= 0.018838060572445543\n",
      "9 Precision mean= 0.9528630738500702\n",
      "9 Precision std= 0.034568072834674876\n",
      "10 Precision mean= 0.8808136183445214\n",
      "10 Precision std= 0.031247136662436076\n",
      "11 Precision mean= 0.9350358907289085\n",
      "11 Precision std= 0.024998532463082656\n",
      "12 Precision mean= 0.9046096937180306\n",
      "12 Precision std= 0.02751995038132959\n",
      "Recall mean of  0  = 0.9999670619235836\n",
      "Recall std of  0  = 0.00014357374649344182\n",
      "Recall mean of  1  = 0.9950843809811827\n",
      "Recall std of  1  = 0.005878255296027442\n",
      "Recall mean of  2  = 0.9902953348028228\n",
      "Recall std of  2  = 0.009663351472692238\n",
      "Recall mean of  3  = 0.9968212220071798\n",
      "Recall std of  3  = 0.004271697939686913\n",
      "Recall mean of  4  = 0.9945774557703363\n",
      "Recall std of  4  = 0.004258668727830598\n",
      "Recall mean of  5  = 0.9986072730844431\n",
      "Recall std of  5  = 0.002913019447836434\n",
      "Recall mean of  6  = 0.9921152276125287\n",
      "Recall std of  6  = 0.006256824009403261\n",
      "Recall mean of  7  = 0.9532197724857661\n",
      "Recall std of  7  = 0.017674403602856895\n",
      "Recall mean of  8  = 0.9001141410839338\n",
      "Recall std of  8  = 0.0363281961082816\n",
      "Recall mean of  9  = 0.8438659523708028\n",
      "Recall std of  9  = 0.04438445783981835\n",
      "Recall mean of  10  = 0.9380911065594386\n",
      "Recall std of  10  = 0.029065070243060712\n",
      "Recall mean of  11  = 0.8281517734654786\n",
      "Recall std of  11  = 0.02513930557754542\n",
      "Recall mean of  12  = 0.984224434705855\n",
      "Recall std of  12  = 0.013841131534062879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x.drop(columns='attack_type')\n",
    "        x.drop(columns='Attack_label')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = LogisticRegression(random_state = rs+1)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce9f42c8d164a339cbda7e4d9352ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 1.0\n",
      "0 Precision std= 0.0\n",
      "1 Precision mean= 0.9656997199487243\n",
      "1 Precision std= 0.011757366302334584\n",
      "2 Precision mean= 0.9862230263093192\n",
      "2 Precision std= 0.007011488430508363\n",
      "3 Precision mean= 0.9966538840260268\n",
      "3 Precision std= 0.003579273615586777\n",
      "4 Precision mean= 0.9932482439185814\n",
      "4 Precision std= 0.00521235321780772\n",
      "5 Precision mean= 1.0\n",
      "5 Precision std= 0.0\n",
      "6 Precision mean= 0.9785019613935922\n",
      "6 Precision std= 0.010082666904614462\n",
      "7 Precision mean= 0.9677889154493101\n",
      "7 Precision std= 0.00772782180136017\n",
      "8 Precision mean= 0.9537367457063086\n",
      "8 Precision std= 0.011461444910340707\n",
      "9 Precision mean= 0.8533055560214062\n",
      "9 Precision std= 0.024128831149708495\n",
      "10 Precision mean= 0.8746801783508842\n",
      "10 Precision std= 0.01903147619286737\n",
      "11 Precision mean= 0.8998460694256879\n",
      "11 Precision std= 0.025524724010747273\n",
      "12 Precision mean= 0.9329077493951395\n",
      "12 Precision std= 0.020335162283407034\n",
      "Recall mean of  0  = 0.9998842057957733\n",
      "Recall std of  0  = 0.00015792424276320249\n",
      "Recall mean of  1  = 0.9767389102669798\n",
      "Recall std of  1  = 0.008151173829898009\n",
      "Recall mean of  2  = 0.956916826927969\n",
      "Recall std of  2  = 0.014923986081912902\n",
      "Recall mean of  3  = 0.9997942386831277\n",
      "Recall std of  3  = 0.0008968927867367601\n",
      "Recall mean of  4  = 0.9997890295358649\n",
      "Recall std of  4  = 0.0009195989332364176\n",
      "Recall mean of  5  = 0.9966296756980777\n",
      "Recall std of  5  = 0.003612720450645117\n",
      "Recall mean of  6  = 0.9845940926341953\n",
      "Recall std of  6  = 0.008050958645759679\n",
      "Recall mean of  7  = 0.9912696099265699\n",
      "Recall std of  7  = 0.006886699134794421\n",
      "Recall mean of  8  = 0.9566529865671409\n",
      "Recall std of  8  = 0.008846606345540027\n",
      "Recall mean of  9  = 0.9265553805725132\n",
      "Recall std of  9  = 0.01740326413944785\n",
      "Recall mean of  10  = 0.850359573948942\n",
      "Recall std of  10  = 0.023409243716027178\n",
      "Recall mean of  11  = 0.8490774332650396\n",
      "Recall std of  11  = 0.03490750821542442\n",
      "Recall mean of  12  = 0.9102810336525053\n",
      "Recall std of  12  = 0.020008929920048844\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x.drop(columns='attack_type')\n",
    "        x.drop(columns='Attack_label')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad6a9ebe595c8604775664ccb6b4edb43e0871577ed89c469f16f6efb723bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
