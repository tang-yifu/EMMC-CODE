{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the repeat experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = [\"Backdoor.csv\" , \"DDoS_HTTP.csv\" , \"DDoS_ICMP.csv\" , \"DDoS_TCP.csv\" , \"DDoS_UDP.csv\"  , \"Password.csv\" , \"Port_Scanning.csv\" ,\"Ransomware.csv\", \"SQL_injection.csv\" , \"Uploading.csv\" , \"Vulnerability_scanner.csv\" ,\"XSS.csv\" ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMMC Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, clfs, p_nodes, gms):\n",
    "    p_x_node = np.zeros((len(x), len(clfs))) # p[x, in node i] = p(x | x in Node i) * p(x in Node i)\n",
    "    p_y_given_node = np.zeros((len(x), len(clfs))) # p[y | x, node i]\n",
    "    for i in range(len(clfs)):\n",
    "        p_x_node[:, i] = p_nodes[i] * np.exp(gms[i].score_samples(x))\n",
    "        p_y_given_node[:, i] = clfs[i].predict_proba(x)[:, 1]\n",
    "    \n",
    "    p_y = p_y_given_node * p_x_node / (np.sum(p_x_node, axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "    p_y = np.hstack([1-np.sum(p_y, axis=-1, keepdims=True), p_y])\n",
    "\n",
    "    return np.argmax(p_y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(input_file)):\n",
    "    data = pd.read_csv(input_file[i])\n",
    "    data['attack_type'] = i + 1\n",
    "    data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "    data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "    if len(data) < 2000:\n",
    "        data_list.append(data)\n",
    "    else:\n",
    "        data0 = data[data['attack_type'].isin([0])]\n",
    "        x1 = data0.sample(n = 1000, random_state = i + 42, axis = 0)\n",
    "        data1 = data[data['attack_type'].isin([i+1])]\n",
    "        x2 = data1.sample(n = 1000, random_state = i + 43, axis = 0)   \n",
    "        data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "        data_list.append(data_selected) \n",
    "for i in range(len(data_list)):\n",
    "    print(len(data_list[i]))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>http.content_length</th>\n",
       "      <th>http.response</th>\n",
       "      <th>tcp.ack</th>\n",
       "      <th>tcp.ack_raw</th>\n",
       "      <th>tcp.checksum</th>\n",
       "      <th>tcp.connection.fin</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.conflag.cleansess</th>\n",
       "      <th>mqtt.conflags</th>\n",
       "      <th>mqtt.hdrflags</th>\n",
       "      <th>mqtt.len</th>\n",
       "      <th>mqtt.msgtype</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1934832499</td>\n",
       "      <td>19721</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24507</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3703765364</td>\n",
       "      <td>46928</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>208620284</td>\n",
       "      <td>44688</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3557485597</td>\n",
       "      <td>42863</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      arp.opcode  arp.hw.size  icmp.checksum  icmp.seq_le  \\\n",
       "1995           0            0              0            0   \n",
       "1996           0            0              0            0   \n",
       "1997           0            0              0            0   \n",
       "1998           0            0              0            0   \n",
       "1999           0            0              0            0   \n",
       "\n",
       "      http.content_length  http.response  tcp.ack  tcp.ack_raw  tcp.checksum  \\\n",
       "1995                    0              0      150   1934832499         19721   \n",
       "1996                    0              0        0            0         24507   \n",
       "1997                    0              0        1   3703765364         46928   \n",
       "1998                    0              0        1    208620284         44688   \n",
       "1999                    0              0        1   3557485597         42863   \n",
       "\n",
       "      tcp.connection.fin  ...  mqtt.conflag.cleansess  mqtt.conflags  \\\n",
       "1995                   0  ...                       0              0   \n",
       "1996                   0  ...                       0              0   \n",
       "1997                   0  ...                       0              0   \n",
       "1998                   0  ...                       0              0   \n",
       "1999                   0  ...                       0              0   \n",
       "\n",
       "      mqtt.hdrflags  mqtt.len  mqtt.msgtype  mqtt.proto_len  mqtt.topic_len  \\\n",
       "1995              0         0             0               0               0   \n",
       "1996              0         0             0               0               0   \n",
       "1997              0         0             0               0               0   \n",
       "1998              0         0             0               0               0   \n",
       "1999              0         0             0               0               0   \n",
       "\n",
       "      mqtt.ver  Attack_label  attack_type  \n",
       "1995         0             1           10  \n",
       "1996         0             1           10  \n",
       "1997         0             1           10  \n",
       "1998         0             1           10  \n",
       "1999         0             1           10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[9].tail()\n",
    "# A sample of what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42 # set the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f014a489dfc42a1b2a41411777dc008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99789ad446054619b70d95f22a12435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total number is  24000\n",
      "The proportion of the nodes are: [0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      " 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clfs = [] # This is going to contain 14 different classifiers\n",
    "n_samples = []\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "for i in tqdm(range(len(input_file))): # reading the data\n",
    "    data0 = data_list[i]\n",
    "    y[i][data0.attack_type == 0] = 0\n",
    "    n_samples.append(len(y[i])) # the number of this node\n",
    "    x = data_list[i]\n",
    "    x = x.drop(columns='attack_type')\n",
    "    x = x.drop(columns='Attack_label')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = rs)\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_test_list.append(x_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    #classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "    # classifier = SVC(kernel = 'rbf', random_state = rs, gamma='scale', max_iter=-1, probability=True)\n",
    "    # classifier.fit(data_list[i], y[i])\n",
    "    classifier = LogisticRegression(max_iter=10000)\n",
    "    classifier.fit(x_train_list[i], y_train_list[i])\n",
    "    clfs.append(classifier)\n",
    "\n",
    "total_n_samples = np.sum(n_samples)\n",
    "print('total number is ',total_n_samples)\n",
    "p_nodes =  np.array(n_samples) / total_n_samples\n",
    "print('The proportion of the nodes are:', p_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_file)):\n",
    "    x_test_list[i] = scaler.transform(x_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.vstack(x_test_list)\n",
    "y_test = np.hstack(y_test_list)\n",
    "x_train = np.vstack(x_train_list)\n",
    "y_train = np.hstack(y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841aab75e71849a2a48e27ebc67e5237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gms = []\n",
    "K = 35 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "for i in tqdm(range(len(input_file))):\n",
    "    x = x_train_list[i]\n",
    "    gm = GaussianMixture(n_components = K).fit(x)  \n",
    "    gms.append(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction = pred(x_test, clfs, p_nodes, gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is: 0.7766666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = prediction == y_test\n",
    "accuracy = np.mean(correct)\n",
    "print('Overall accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [0.9880025940337225, 0.7695473251028807, 0.30864197530864196, 0.9958847736625515, 0.7860082304526749, 1.0, 0.17695473251028807, 0.7366255144032922, 0.5925925925925926, 0.13580246913580246, 0.18930041152263374, 0.7860082304526749, 0.16049382716049382]\n",
      "Recalls: [0.7410019455252919, 0.9166666666666666, 0.7211538461538461, 1.0, 0.7047970479704797, 1.0, 0.8431372549019608, 0.662962962962963, 0.9230769230769231, 0.6111111111111112, 0.7931034482758621, 0.9744897959183674, 1.0]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[y_test==i]) == 0:\n",
    "        accs.append(0)\n",
    "    else:\n",
    "        accs.append(np.mean(correct[y_test==i]))\n",
    "print('Accuracies:', accs)\n",
    "recalls = []\n",
    "for i in range(len(input_file)+1):\n",
    "    if len(correct[prediction==i]) == 0:\n",
    "        recalls.append(0)\n",
    "    else:\n",
    "        recalls.append(np.mean(correct[prediction==i]))\n",
    "print('Recalls:', recalls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the test:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMMC: Binary = LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b163c84a538c4757afe463d8aa195537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latex Precisions\n",
      "71.84 \\% ( 0.01 ) &\n",
      "92.16 \\% ( 0.05 ) &\n",
      "83.02 \\% ( 0.13 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "73.04 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "82.01 \\% ( 0.06 ) &\n",
      "67.4 \\% ( 0.04 ) &\n",
      "88.4 \\% ( 0.06 ) &\n",
      "80.41 \\% ( 0.13 ) &\n",
      "72.72 \\% ( 0.12 ) &\n",
      "96.79 \\% ( 0.02 ) &\n",
      "92.26 \\% ( 0.09 ) &\n",
      "Latex precs\n",
      "99.1 \\% ( 0.0 ) &\n",
      "71.3 \\% ( 0.04 ) &\n",
      "33.81 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "78.02 \\% ( 0.03 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "15.12 \\% ( 0.03 ) &\n",
      "72.68 \\% ( 0.04 ) &\n",
      "46.61 \\% ( 0.09 ) &\n",
      "14.31 \\% ( 0.03 ) &\n",
      "17.59 \\% ( 0.03 ) &\n",
      "78.87 \\% ( 0.03 ) &\n",
      "14.37 \\% ( 0.04 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "\n",
    "        # Random Pick:\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "\n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        # until now, we totally dropped the attack labels\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = LogisticRegression(random_state=epoch)\n",
    "        #classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "    # Finally, we train our GMM here\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 30 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    recs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            recs.append(0)\n",
    "        else:\n",
    "            recs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = recs\n",
    "\n",
    "    precs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            precs.append(0)\n",
    "        else:\n",
    "            precs.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = precs\n",
    "# For Latex:\n",
    "\n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary = KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b36ab3066244d3bb76cdf0cfd146fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latex Precisions\n",
      "73.18 \\% ( 0.01 ) &\n",
      "91.36 \\% ( 0.06 ) &\n",
      "78.79 \\% ( 0.12 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "77.64 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "78.08 \\% ( 0.1 ) &\n",
      "65.66 \\% ( 0.03 ) &\n",
      "76.79 \\% ( 0.06 ) &\n",
      "77.46 \\% ( 0.1 ) &\n",
      "70.68 \\% ( 0.1 ) &\n",
      "98.3 \\% ( 0.01 ) &\n",
      "82.33 \\% ( 0.14 ) &\n",
      "Latex Recalls\n",
      "98.83 \\% ( 0.0 ) &\n",
      "76.33 \\% ( 0.03 ) &\n",
      "34.76 \\% ( 0.03 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "74.52 \\% ( 0.04 ) &\n",
      "99.74 \\% ( 0.0 ) &\n",
      "14.3 \\% ( 0.02 ) &\n",
      "77.9 \\% ( 0.04 ) &\n",
      "57.65 \\% ( 0.09 ) &\n",
      "15.41 \\% ( 0.03 ) &\n",
      "16.62 \\% ( 0.03 ) &\n",
      "78.36 \\% ( 0.02 ) &\n",
      "15.42 \\% ( 0.02 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "\n",
    "        # Random Pick:\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "\n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        # until now, we totally dropped the attack labels\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "    # Finally, we train our GMM here\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 30 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    recs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            recs.append(0)\n",
    "        else:\n",
    "            recs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = recs\n",
    "\n",
    "    precs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            precs.append(0)\n",
    "        else:\n",
    "            precs.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = precs\n",
    "# For Latex:\n",
    "\n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c8dd24c28d4fc094b29998a40cb61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latex Precisions\n",
      "72.75 \\% ( 0.01 ) &\n",
      "90.44 \\% ( 0.07 ) &\n",
      "74.55 \\% ( 0.15 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "77.14 \\% ( 0.05 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "83.26 \\% ( 0.06 ) &\n",
      "65.52 \\% ( 0.04 ) &\n",
      "75.16 \\% ( 0.08 ) &\n",
      "82.44 \\% ( 0.11 ) &\n",
      "70.2 \\% ( 0.1 ) &\n",
      "96.94 \\% ( 0.03 ) &\n",
      "84.75 \\% ( 0.17 ) &\n",
      "Latex Recalls\n",
      "98.63 \\% ( 0.01 ) &\n",
      "74.73 \\% ( 0.05 ) &\n",
      "33.83 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "74.9 \\% ( 0.05 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "13.84 \\% ( 0.03 ) &\n",
      "78.44 \\% ( 0.06 ) &\n",
      "51.73 \\% ( 0.1 ) &\n",
      "14.54 \\% ( 0.03 ) &\n",
      "16.57 \\% ( 0.03 ) &\n",
      "78.94 \\% ( 0.02 ) &\n",
      "14.19 \\% ( 0.04 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "\n",
    "        # Random Pick:\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "\n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        # until now, we totally dropped the attack labels\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 41 + epoch, gamma='scale',max_iter=-1, probability=True)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "    # Finally, we train our GMM here\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 30 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    recs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            recs.append(0)\n",
    "        else:\n",
    "            recs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = recs\n",
    "\n",
    "    precs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            precs.append(0)\n",
    "        else:\n",
    "            precs.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = precs\n",
    "# For Latex:\n",
    "\n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c89a16856a5422d985449d94300d94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latex Precisions\n",
      "74.98 \\% ( 0.02 ) &\n",
      "94.51 \\% ( 0.05 ) &\n",
      "71.11 \\% ( 0.1 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "98.2 \\% ( 0.03 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "71.92 \\% ( 0.16 ) &\n",
      "69.45 \\% ( 0.04 ) &\n",
      "79.61 \\% ( 0.05 ) &\n",
      "71.16 \\% ( 0.12 ) &\n",
      "68.7 \\% ( 0.1 ) &\n",
      "98.3 \\% ( 0.02 ) &\n",
      "67.95 \\% ( 0.16 ) &\n",
      "Latex Recalls\n",
      "99.11 \\% ( 0.0 ) &\n",
      "78.14 \\% ( 0.04 ) &\n",
      "35.7 \\% ( 0.04 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "78.42 \\% ( 0.07 ) &\n",
      "99.9 \\% ( 0.0 ) &\n",
      "16.39 \\% ( 0.03 ) &\n",
      "92.46 \\% ( 0.03 ) &\n",
      "66.04 \\% ( 0.12 ) &\n",
      "20.85 \\% ( 0.05 ) &\n",
      "17.14 \\% ( 0.03 ) &\n",
      "78.48 \\% ( 0.02 ) &\n",
      "15.49 \\% ( 0.03 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1 \n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "\n",
    "        # Random Pick:\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "\n",
    "    #Training\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type' , 'Attack_label'])\n",
    "        # until now, we totally dropped the attack labels\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = rs)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "    # Finally, we train our GMM here\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    K = 30 # The number of the components of the GMM, one could change it by analyzing the graph\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "\n",
    "    # Test:\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "    correct = prediction == y_test\n",
    "\n",
    "    recs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            recs.append(0)\n",
    "        else:\n",
    "            recs.append(np.mean(correct[y_test==i]))\n",
    "    \n",
    "    recall[epoch,:] = recs\n",
    "\n",
    "    precs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            precs.append(0)\n",
    "        else:\n",
    "            precs.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = precs\n",
    "# For Latex:\n",
    "\n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full data classification:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18817b6abc3944ffa359d0032d8dfbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.6972837585359323\n",
      "0 Precision std= 0.018303512465238474\n",
      "1 Precision mean= 0.4038418216092398\n",
      "1 Precision std= 0.03937783178100957\n",
      "2 Precision mean= 0.527915416028818\n",
      "2 Precision std= 0.04127416591604016\n",
      "3 Precision mean= 0.9986146263616664\n",
      "3 Precision std= 0.002886313340697261\n",
      "4 Precision mean= 0.7418605439876964\n",
      "4 Precision std= 0.06688044980148687\n",
      "5 Precision mean= 0.9993941852425012\n",
      "5 Precision std= 0.001442356923587901\n",
      "6 Precision mean= 0.8206673433801329\n",
      "6 Precision std= 0.14557647066506124\n",
      "7 Precision mean= 0.6777563878212962\n",
      "7 Precision std= 0.06208623882638545\n",
      "8 Precision mean= 0.3309501884819273\n",
      "8 Precision std= 0.05465581264530748\n",
      "9 Precision mean= 0.49680769876716424\n",
      "9 Precision std= 0.0770678917169437\n",
      "10 Precision mean= 0.6444430145872899\n",
      "10 Precision std= 0.1020292679184349\n",
      "11 Precision mean= 0.8785379113370929\n",
      "11 Precision std= 0.01608047448712708\n",
      "12 Precision mean= 0.0\n",
      "12 Precision std= 0.0\n",
      "Recall mean of  0  = 0.9908181767574199\n",
      "Recall std of  0  = 0.0017703357947003071\n",
      "Recall mean of  1  = 0.3016254061064673\n",
      "Recall std of  1  = 0.041211656196641\n",
      "Recall mean of  2  = 0.257612225628053\n",
      "Recall std of  2  = 0.0265342020761714\n",
      "Recall mean of  3  = 0.996808585984108\n",
      "Recall std of  3  = 0.004131233872392726\n",
      "Recall mean of  4  = 0.7542956926455091\n",
      "Recall std of  4  = 0.08943365833823023\n",
      "Recall mean of  5  = 0.9818004593637101\n",
      "Recall std of  5  = 0.008751467899456557\n",
      "Recall mean of  6  = 0.06616519043239974\n",
      "Recall std of  6  = 0.019907773303973083\n",
      "Recall mean of  7  = 0.7054663218589996\n",
      "Recall std of  7  = 0.10268883715087744\n",
      "Recall mean of  8  = 0.1342110167954737\n",
      "Recall std of  8  = 0.027855741946911896\n",
      "Recall mean of  9  = 0.0931470034878791\n",
      "Recall std of  9  = 0.017097951857740445\n",
      "Recall mean of  10  = 0.06495529219292767\n",
      "Recall std of  10  = 0.015933903753571288\n",
      "Recall mean of  11  = 0.7438119710193607\n",
      "Recall std of  11  = 0.032016280750955056\n",
      "Recall mean of  12  = 0.0\n",
      "Recall std of  12  = 0.0\n",
      "Latex Precisions\n",
      "69.73 \\% ( 0.02 ) &\n",
      "40.38 \\% ( 0.04 ) &\n",
      "52.79 \\% ( 0.04 ) &\n",
      "99.86 \\% ( 0.0 ) &\n",
      "74.19 \\% ( 0.07 ) &\n",
      "99.94 \\% ( 0.0 ) &\n",
      "82.07 \\% ( 0.15 ) &\n",
      "67.78 \\% ( 0.06 ) &\n",
      "33.1 \\% ( 0.05 ) &\n",
      "49.68 \\% ( 0.08 ) &\n",
      "64.44 \\% ( 0.1 ) &\n",
      "87.85 \\% ( 0.02 ) &\n",
      "0.0 \\% ( 0.0 ) &\n",
      "Latex Recalls\n",
      "99.08 \\% ( 0.0 ) &\n",
      "30.16 \\% ( 0.04 ) &\n",
      "25.76 \\% ( 0.03 ) &\n",
      "99.68 \\% ( 0.0 ) &\n",
      "75.43 \\% ( 0.09 ) &\n",
      "98.18 \\% ( 0.01 ) &\n",
      "6.62 \\% ( 0.02 ) &\n",
      "70.55 \\% ( 0.1 ) &\n",
      "13.42 \\% ( 0.03 ) &\n",
      "9.31 \\% ( 0.02 ) &\n",
      "6.5 \\% ( 0.02 ) &\n",
      "74.38 \\% ( 0.03 ) &\n",
      "0.0 \\% ( 0.0 ) &\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = LogisticRegression(random_state = epoch)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1edf77c3b884eba900a4edf088326a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7480887457555241\n",
      "0 Precision std= 0.016593364965375768\n",
      "1 Precision mean= 0.5419835949755898\n",
      "1 Precision std= 0.028222316490708946\n",
      "2 Precision mean= 0.48929620907957927\n",
      "2 Precision std= 0.03691055437913015\n",
      "3 Precision mean= 0.9962746197645146\n",
      "3 Precision std= 0.0035993217922279234\n",
      "4 Precision mean= 0.7095183709391046\n",
      "4 Precision std= 0.02224567467586668\n",
      "5 Precision mean= 0.9980016885928794\n",
      "5 Precision std= 0.0032779936852438754\n",
      "6 Precision mean= 0.33120715558394476\n",
      "6 Precision std= 0.07247355892328458\n",
      "7 Precision mean= 0.7035021999864176\n",
      "7 Precision std= 0.027260892697049437\n",
      "8 Precision mean= 0.687061573274929\n",
      "8 Precision std= 0.03701352708301233\n",
      "9 Precision mean= 0.4018288689149343\n",
      "9 Precision std= 0.05541492568783841\n",
      "10 Precision mean= 0.38269939622721905\n",
      "10 Precision std= 0.04870929614910959\n",
      "11 Precision mean= 0.8470160659294919\n",
      "11 Precision std= 0.022105486316875343\n",
      "12 Precision mean= 0.20763897919367\n",
      "12 Precision std= 0.05495894102530598\n",
      "Recall mean of  0  = 0.9280691832872051\n",
      "Recall std of  0  = 0.007058699656318592\n",
      "Recall mean of  1  = 0.7992558856806952\n",
      "Recall std of  1  = 0.028393077258956147\n",
      "Recall mean of  2  = 0.3044100307612331\n",
      "Recall std of  2  = 0.02768506884606984\n",
      "Recall mean of  3  = 0.9990066285834525\n",
      "Recall std of  3  = 0.0017220456921451172\n",
      "Recall mean of  4  = 0.7940252370081601\n",
      "Recall std of  4  = 0.031467300628542874\n",
      "Recall mean of  5  = 0.9942176529294671\n",
      "Recall std of  5  = 0.004410811107993635\n",
      "Recall mean of  6  = 0.09912380311937018\n",
      "Recall std of  6  = 0.019842336749312506\n",
      "Recall mean of  7  = 0.6231735483325813\n",
      "Recall std of  7  = 0.03477458399719955\n",
      "Recall mean of  8  = 0.6566896438268433\n",
      "Recall std of  8  = 0.02754998361540661\n",
      "Recall mean of  9  = 0.12796721877699255\n",
      "Recall std of  9  = 0.018716789830501304\n",
      "Recall mean of  10  = 0.13475606641954369\n",
      "Recall std of  10  = 0.015647226646880383\n",
      "Recall mean of  11  = 0.7507963497045222\n",
      "Recall std of  11  = 0.029387809538135173\n",
      "Recall mean of  12  = 0.03835848954510491\n",
      "Recall std of  12  = 0.011934061302023258\n",
      "Latex Precisions\n",
      "74.81 \\% ( 0.02 ) &\n",
      "54.2 \\% ( 0.03 ) &\n",
      "48.93 \\% ( 0.04 ) &\n",
      "99.63 \\% ( 0.0 ) &\n",
      "70.95 \\% ( 0.02 ) &\n",
      "99.8 \\% ( 0.0 ) &\n",
      "33.12 \\% ( 0.07 ) &\n",
      "70.35 \\% ( 0.03 ) &\n",
      "68.71 \\% ( 0.04 ) &\n",
      "40.18 \\% ( 0.06 ) &\n",
      "38.27 \\% ( 0.05 ) &\n",
      "84.7 \\% ( 0.02 ) &\n",
      "20.76 \\% ( 0.05 ) &\n",
      "Latex Recalls\n",
      "92.81 \\% ( 0.01 ) &\n",
      "79.93 \\% ( 0.03 ) &\n",
      "30.44 \\% ( 0.03 ) &\n",
      "99.9 \\% ( 0.0 ) &\n",
      "79.4 \\% ( 0.03 ) &\n",
      "99.42 \\% ( 0.0 ) &\n",
      "9.91 \\% ( 0.02 ) &\n",
      "62.32 \\% ( 0.03 ) &\n",
      "65.67 \\% ( 0.03 ) &\n",
      "12.8 \\% ( 0.02 ) &\n",
      "13.48 \\% ( 0.02 ) &\n",
      "75.08 \\% ( 0.03 ) &\n",
      "3.84 \\% ( 0.01 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be479b79c7a4840bacae0fe7656acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.90589853888143\n",
      "0 Precision std= 0.006580613078238353\n",
      "1 Precision mean= 0.9366169078043483\n",
      "1 Precision std= 0.014485752411366626\n",
      "2 Precision mean= 0.6415434786173213\n",
      "2 Precision std= 0.02361746883293334\n",
      "3 Precision mean= 0.9995997694672131\n",
      "3 Precision std= 0.0012010757582163777\n",
      "4 Precision mean= 0.9997863247863247\n",
      "4 Precision std= 0.0009313886631497133\n",
      "5 Precision mean= 0.9985836138618666\n",
      "5 Precision std= 0.0030048836765764414\n",
      "6 Precision mean= 0.8113134230011194\n",
      "6 Precision std= 0.02879803513308709\n",
      "7 Precision mean= 0.8307061444642816\n",
      "7 Precision std= 0.021755638790884855\n",
      "8 Precision mean= 0.8064086597578095\n",
      "8 Precision std= 0.03755852021785365\n",
      "9 Precision mean= 0.7592343586622707\n",
      "9 Precision std= 0.034898868632500524\n",
      "10 Precision mean= 0.6729387806635958\n",
      "10 Precision std= 0.025266515705357595\n",
      "11 Precision mean= 0.9636122397297683\n",
      "11 Precision std= 0.010653571517084418\n",
      "12 Precision mean= 0.8107260470177945\n",
      "12 Precision std= 0.04936101341480204\n",
      "Recall mean of  0  = 0.9636885952216276\n",
      "Recall std of  0  = 0.0035420455182158714\n",
      "Recall mean of  1  = 0.8948090876283402\n",
      "Recall std of  1  = 0.01439842835802929\n",
      "Recall mean of  2  = 0.6227273876455299\n",
      "Recall std of  2  = 0.04357497818069272\n",
      "Recall mean of  3  = 0.9995950354958765\n",
      "Recall std of  3  = 0.001215070507024462\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n",
      "Recall mean of  5  = 0.9993957311019575\n",
      "Recall std of  5  = 0.0014387841144611298\n",
      "Recall mean of  6  = 0.640199203963274\n",
      "Recall std of  6  = 0.021887937176097536\n",
      "Recall mean of  7  = 0.928769021453841\n",
      "Recall std of  7  = 0.027305688790368314\n",
      "Recall mean of  8  = 0.8370079270019595\n",
      "Recall std of  8  = 0.03448589909505018\n",
      "Recall mean of  9  = 0.6697863814497019\n",
      "Recall std of  9  = 0.03556815334583653\n",
      "Recall mean of  10  = 0.5035863702099268\n",
      "Recall std of  10  = 0.028149844368548546\n",
      "Recall mean of  11  = 0.8920696903962174\n",
      "Recall std of  11  = 0.02289417528921394\n",
      "Recall mean of  12  = 0.6444093277275696\n",
      "Recall std of  12  = 0.02349175744183058\n",
      "Latex Precisions\n",
      "90.59 \\% ( 0.01 ) &\n",
      "93.66 \\% ( 0.01 ) &\n",
      "64.15 \\% ( 0.02 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "99.98 \\% ( 0.0 ) &\n",
      "99.86 \\% ( 0.0 ) &\n",
      "81.13 \\% ( 0.03 ) &\n",
      "83.07 \\% ( 0.02 ) &\n",
      "80.64 \\% ( 0.04 ) &\n",
      "75.92 \\% ( 0.03 ) &\n",
      "67.29 \\% ( 0.03 ) &\n",
      "96.36 \\% ( 0.01 ) &\n",
      "81.07 \\% ( 0.05 ) &\n",
      "Latex Recalls\n",
      "96.37 \\% ( 0.0 ) &\n",
      "89.48 \\% ( 0.01 ) &\n",
      "62.27 \\% ( 0.04 ) &\n",
      "99.96 \\% ( 0.0 ) &\n",
      "100.0 \\% ( 0.0 ) &\n",
      "99.94 \\% ( 0.0 ) &\n",
      "64.02 \\% ( 0.02 ) &\n",
      "92.88 \\% ( 0.03 ) &\n",
      "83.7 \\% ( 0.03 ) &\n",
      "66.98 \\% ( 0.04 ) &\n",
      "50.36 \\% ( 0.03 ) &\n",
      "89.21 \\% ( 0.02 ) &\n",
      "64.44 \\% ( 0.02 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = rs + i)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2b3b057d41442bb4a56de94b4c0b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Precision mean= 0.7018560977899171\n",
      "0 Precision std= 0.01823249266520187\n",
      "1 Precision mean= 0.4211066053685132\n",
      "1 Precision std= 0.0765853174386387\n",
      "2 Precision mean= 0.4711404864775954\n",
      "2 Precision std= 0.04005645546023005\n",
      "3 Precision mean= 0.9966277564913211\n",
      "3 Precision std= 0.003125778608181801\n",
      "4 Precision mean= 0.9524019413331141\n",
      "4 Precision std= 0.10602799176390985\n",
      "5 Precision mean= 0.8435448069392588\n",
      "5 Precision std= 0.054839739133382644\n",
      "6 Precision mean= 0.9898793363499246\n",
      "6 Precision std= 0.024262373302972276\n",
      "7 Precision mean= 0.5981090682805664\n",
      "7 Precision std= 0.03677171817651272\n",
      "8 Precision mean= 0.4164394749442808\n",
      "8 Precision std= 0.10324497402064176\n",
      "9 Precision mean= 0.8020319943349445\n",
      "9 Precision std= 0.1013041515008211\n",
      "10 Precision mean= 0.7064000182877034\n",
      "10 Precision std= 0.06939506059932997\n",
      "11 Precision mean= 0.8770294682065561\n",
      "11 Precision std= 0.015897778270731723\n",
      "12 Precision mean= 0.0\n",
      "12 Precision std= 0.0\n",
      "Recall mean of  0  = 0.9866006485198149\n",
      "Recall std of  0  = 0.002320196950575675\n",
      "Recall mean of  1  = 0.28408196019603116\n",
      "Recall std of  1  = 0.030519732827806217\n",
      "Recall mean of  2  = 0.2580243347423014\n",
      "Recall std of  2  = 0.026208281463225964\n",
      "Recall mean of  3  = 0.9933819444139977\n",
      "Recall std of  3  = 0.00662569186159084\n",
      "Recall mean of  4  = 0.5954007077241281\n",
      "Recall std of  4  = 0.08461242824500141\n",
      "Recall mean of  5  = 0.9960707512049524\n",
      "Recall std of  5  = 0.004508308477940797\n",
      "Recall mean of  6  = 0.047779430065307396\n",
      "Recall std of  6  = 0.016476639739395556\n",
      "Recall mean of  7  = 0.8844025883404452\n",
      "Recall std of  7  = 0.08802232365381094\n",
      "Recall mean of  8  = 0.1846895301446546\n",
      "Recall std of  8  = 0.08550658472809052\n",
      "Recall mean of  9  = 0.0931470034878791\n",
      "Recall std of  9  = 0.017097951857740445\n",
      "Recall mean of  10  = 0.09832010717285841\n",
      "Recall std of  10  = 0.014088050086836945\n",
      "Recall mean of  11  = 0.7322454651332235\n",
      "Recall std of  11  = 0.03244708705080332\n",
      "Recall mean of  12  = 0.0\n",
      "Recall std of  12  = 0.0\n",
      "Latex Precisions\n",
      "70.19 \\% ( 0.02 ) &\n",
      "42.11 \\% ( 0.08 ) &\n",
      "47.11 \\% ( 0.04 ) &\n",
      "99.66 \\% ( 0.0 ) &\n",
      "95.24 \\% ( 0.11 ) &\n",
      "84.35 \\% ( 0.05 ) &\n",
      "98.99 \\% ( 0.02 ) &\n",
      "59.81 \\% ( 0.04 ) &\n",
      "41.64 \\% ( 0.1 ) &\n",
      "80.2 \\% ( 0.1 ) &\n",
      "70.64 \\% ( 0.07 ) &\n",
      "87.7 \\% ( 0.02 ) &\n",
      "0.0 \\% ( 0.0 ) &\n",
      "Latex Recalls\n",
      "98.66 \\% ( 0.0 ) &\n",
      "28.41 \\% ( 0.03 ) &\n",
      "25.8 \\% ( 0.03 ) &\n",
      "99.34 \\% ( 0.01 ) &\n",
      "59.54 \\% ( 0.08 ) &\n",
      "99.61 \\% ( 0.0 ) &\n",
      "4.78 \\% ( 0.02 ) &\n",
      "88.44 \\% ( 0.09 ) &\n",
      "18.47 \\% ( 0.09 ) &\n",
      "9.31 \\% ( 0.02 ) &\n",
      "9.83 \\% ( 0.01 ) &\n",
      "73.22 \\% ( 0.03 ) &\n",
      "0.0 \\% ( 0.0 ) &\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "\n",
    "recall , precision = np.ones((num_epoch,len(input_file)+1))*0 , np.ones((num_epoch,len(input_file)+1))*0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    #Data Process\n",
    "    columns_to_drop_bi = ['Attack_type']\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data['attack_type'] = i + 1\n",
    "        data.loc[data.Attack_type == 'Normal', 'attack_type'] = 0\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        if len(data) < 2000:\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            data0 = data[data['attack_type'].isin([0])]\n",
    "            x1 = data0.sample(n = 1000, random_state = i + epoch, axis = 0)\n",
    "            data1 = data[data['attack_type'].isin([i+1])]\n",
    "            x2 = data1.sample(n = 1000, random_state = i + epoch + 1, axis = 0)   \n",
    "            data_selected = pd.concat([x1, x2], ignore_index=True)  \n",
    "            data_list.append(data_selected)  \n",
    "    #Training\n",
    "\n",
    "\n",
    "    clfs = [] # This is going to contain 12 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = data_list[i]\n",
    "        y[i][data0.attack_type == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns=['attack_type','Attack_label'])\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = 0.25, random_state = epoch+40)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    y_train_full = np.hstack(y_train_list)\n",
    "    x_test_full = pd.concat(x_test_list, ignore_index=True)\n",
    "    #y_test_full = pd.concat(y_test_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "    x_train_full = scaler.transform(x_train_full)\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "    classifier.fit(x_train_full, y_train_full)\n",
    "\n",
    "    x_test_full = scaler.transform(x_test_full)\n",
    "\n",
    "    #x_test = np.vstack(x_test_list)\n",
    "    y_test_full = np.hstack(y_test_list)\n",
    "   # x_train = np.vstack(x_train_list)\n",
    "    \n",
    "\n",
    "\n",
    "    prediction = classifier.predict(x_test_full)\n",
    "    correct = prediction == y_test_full\n",
    "\n",
    "    rec = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test_full==i]) == 0:\n",
    "            rec.append(0)\n",
    "        else:\n",
    "            rec.append(np.mean(correct[y_test_full==i]))\n",
    "    \n",
    "    recall[epoch,:] = rec\n",
    "      \n",
    "\n",
    "    pr = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            pr.append(0)\n",
    "        else:\n",
    "            pr.append(np.mean(correct[prediction==i]))\n",
    "    \n",
    "    precision[epoch,:] = pr\n",
    "    \n",
    "for i in range(len(input_file)+1):\n",
    "      print(i,'Precision mean=',np.mean(precision[:,i]))\n",
    "      print(i,'Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "      \n",
    "print('Latex Precisions')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(precision[:,i]),2),'\\% (',round(np.std(precision[:,i]),2),') &' )\n",
    "\n",
    "\n",
    "print('Latex Recalls')\n",
    "for i in range(len(input_file)+1):\n",
    "      print(round(100 * np.mean(recall[:,i]),2),'\\% (',round(np.std(recall[:,i]),2),') &' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad6a9ebe595c8604775664ccb6b4edb43e0871577ed89c469f16f6efb723bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
