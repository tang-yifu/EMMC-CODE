{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we first do the PCA and then check the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = [ \"biflow_sparta.csv\", \"biflow_scan_sU.csv\", 'biflow_scan_A.csv', \"biflow_mqtt_bruteforce.csv\"]\n",
    "columns_to_drop_bi = ['proto', 'ip_src', 'ip_dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, clfs, p_nodes, gms):\n",
    "    p_x_node = np.zeros((len(x), len(clfs))) \n",
    "    p_y_given_node = np.zeros((len(x), len(clfs))) \n",
    "    for i in range(len(clfs)):\n",
    "        p_x_node[:, i] = p_nodes[i] * np.exp(gms[i].score_samples(x))\n",
    "        p_y_given_node[:, i] = clfs[i].predict_proba(x)[:, 1]\n",
    "    \n",
    "    p_y = p_y_given_node * p_x_node / (np.sum(p_x_node, axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "    p_y = np.hstack([1-np.sum(p_y, axis=-1, keepdims=True), p_y])\n",
    "\n",
    "    return np.argmax(p_y, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fed_SVC_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 41, gamma='scale',max_iter=-1, probability=True)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Fed_SVC_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8effd361ca0e44579e07cd63dd172d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.99903329216703\n",
      "Precision std= 0.0030676796325696167\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9999195907445187\n",
      "Precision std= 0.00010546132833212416\n",
      "Precision mean= 0.9971699887458062\n",
      "Precision std= 0.0006486508423184991\n",
      "Precision mean= 0.9706960413000914\n",
      "Precision std= 0.002058017383807487\n",
      "Recall mean of  0  = 0.9952855269297813\n",
      "Recall std of  0  = 0.0003815112715059807\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.9993389170648916\n",
      "Recall std of  2  = 0.00023414303200288385\n",
      "Recall mean of  3  = 0.9959878871320514\n",
      "Recall std of  3  = 0.014956630369360343\n",
      "Recall mean of  4  = 0.9999573257467993\n",
      "Recall std of  4  = 0.0001860127571923506\n"
     ]
    }
   ],
   "source": [
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628529b48ad843d7beb8beed209d1fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9997851119324409\n",
      "Precision std= 9.692096438756425e-05\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9999196673279016\n",
      "Precision std= 0.00010538857390851933\n",
      "Precision mean= 0.9999698950475733\n",
      "Precision std= 7.166494843614127e-05\n",
      "Precision mean= 0.9707098572713541\n",
      "Precision std= 0.002057137975372695\n",
      "Recall mean of  0  = 0.9958311047250927\n",
      "Recall std of  0  = 0.0003114342175256897\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.9994554902436062\n",
      "Recall std of  2  = 0.0003163879362673795\n",
      "Recall mean of  3  = 0.9995183904566783\n",
      "Recall std of  3  = 0.0003073134819531641\n",
      "Recall mean of  4  = 0.9999857712009106\n",
      "Recall std of  4  = 6.202189731844314e-05\n"
     ]
    }
   ],
   "source": [
    "def Fed_knn_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = KNeighborsClassifier()\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Fed_knn_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0649b7edfc15448186209dbda3cfb96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9876506085171499\n",
      "Precision std= 0.003785463910724846\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9987506024068207\n",
      "Precision std= 0.0021311291362336015\n",
      "Precision mean= 0.9972498269214393\n",
      "Precision std= 0.0006324387229597188\n",
      "Precision mean= 0.9706960413000914\n",
      "Precision std= 0.002058017383807487\n",
      "Recall mean of  0  = 0.9949905705822607\n",
      "Recall std of  0  = 0.0006040269834236191\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.9967037314634022\n",
      "Recall std of  2  = 0.0067733042221125245\n",
      "Recall mean of  3  = 0.9438932142488138\n",
      "Recall std of  3  = 0.01899538541178398\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n"
     ]
    }
   ],
   "source": [
    "def Fed_LR_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = LogisticRegression(max_iter=100)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Fed_LR_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294d3dab9c7a41198d0d48179c528c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9999882879407587\n",
      "Precision std= 2.1739315276567074e-05\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9999284812139357\n",
      "Precision std= 0.00010436471842279804\n",
      "Precision mean= 0.9999798892840884\n",
      "Precision std= 6.033232701048584e-05\n",
      "Precision mean= 0.9707235715550114\n",
      "Precision std= 0.002063439536603464\n",
      "Recall mean of  0  = 0.9958455362052823\n",
      "Recall std of  0  = 0.0003127049646664392\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.999991111111111\n",
      "Recall std of  2  = 3.8745768387040074e-05\n",
      "Recall mean of  3  = 0.9999699879055823\n",
      "Recall std of  3  = 7.144320011612166e-05\n",
      "Recall mean of  4  = 0.9999149971911665\n",
      "Recall std of  4  = 0.00015770601603145597\n"
     ]
    }
   ],
   "source": [
    "def Fed_RF_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = random_state)\n",
    "        classifier.fit(x_train_list[i], y_train_list[i])\n",
    "        clfs.append(classifier)\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gms = []\n",
    "    for i in range(len(input_file)):\n",
    "        x = x_train_list[i]\n",
    "        gm = GaussianMixture(n_components = K).fit(x)  \n",
    "        gms.append(gm)\n",
    "\n",
    "    prediction = pred(x_test, clfs, p_nodes, gms)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Fed_RF_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the Full data after PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR Centrailized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c422eb676b0a4a439ad960c53a9b2e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9943426263658429\n",
      "Precision std= 0.0007108911523247878\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.7939042957159888\n",
      "Precision std= 0.005879766072339707\n",
      "Precision mean= 0.852208907330481\n",
      "Precision std= 0.043490327784445414\n",
      "Precision mean= 0.9705177235955199\n",
      "Precision std= 0.0021283581189608203\n",
      "Recall mean of  0  = 0.9275315864129258\n",
      "Recall std of  0  = 0.007884172304582547\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.9971150517067612\n",
      "Recall std of  2  = 0.0025217858766502376\n",
      "Recall mean of  3  = 0.9684834407362739\n",
      "Recall std of  3  = 0.002686844980200555\n",
      "Recall mean of  4  = 0.9998865237586998\n",
      "Recall std of  4  = 0.0002087984970708694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def Cen_LR_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "\n",
    "    classifier = LogisticRegression(max_iter=100)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Cen_LR_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eda93f9d8b5435c94997a7b743f2185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9997091104715137\n",
      "Precision std= 0.0002241402390449872\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9709164271564521\n",
      "Precision std= 0.0020706295436132374\n",
      "Recall mean of  0  = 0.9958831158051403\n",
      "Recall std of  0  = 0.00030121514378944305\n",
      "Recall mean of  1  = 0.9998162351716072\n",
      "Recall std of  1  = 0.0002867443403187562\n",
      "Recall mean of  2  = 0.9997056419450144\n",
      "Recall std of  2  = 0.00031621710982571237\n",
      "Recall mean of  3  = 0.999749592470548\n",
      "Recall std of  3  = 0.00025205363253831065\n",
      "Recall mean of  4  = 0.9988977583771719\n",
      "Recall std of  4  = 0.0012227471013892778\n"
     ]
    }
   ],
   "source": [
    "def Cen_knn_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Cen_knn_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ed6d5bbb5c47eea7947875c56bf1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9975841412803736\n",
      "Precision std= 0.0003330946007789417\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.980167749391337\n",
      "Precision std= 0.001653922354897871\n",
      "Recall mean of  0  = 0.9971847429895095\n",
      "Recall std of  0  = 0.00024396573132938096\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 1.0\n",
      "Recall std of  2  = 0.0\n",
      "Recall mean of  3  = 0.9999899819675416\n",
      "Recall std of  3  = 4.36675910993865e-05\n",
      "Recall mean of  4  = 0.9829476074846781\n",
      "Recall std of  4  = 0.0023514263774262576\n"
     ]
    }
   ],
   "source": [
    "def Cen_RF_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "\n",
    "    classifier = RandomForestClassifier(random_state = random_state)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Cen_RF_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387013d2cd65426b9485ee899daf19a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Cen_SVC_learn(input_file, columns_to_drop_bi, random_state,  K, test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data.drop(columns='is_attack')\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    #we do PCA here\n",
    "\n",
    "    total_data = pd.concat(data_list, ignore_index=True)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(total_data)\n",
    "    for i in range(len(input_file)):\n",
    "        data_list[i] = pca.transform(data_list[i])\n",
    "\n",
    "    clfs = [] # This is going to contain 14 different classifiers\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_attack == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = np.concatenate(x_train_list,axis=0)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "\n",
    "    classifier = SVC(kernel = 'rbf', random_state = random_state + 1, gamma='scale',max_iter=-1, probability=True)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "def test(epoch, input_file, columns_to_drop_bi, K):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Cen_SVC_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs, K=K)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n",
    "\n",
    "test(epoch=20, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi, K=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad6a9ebe595c8604775664ccb6b4edb43e0871577ed89c469f16f6efb723bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
